---
title: "Re-examining cross-cultural similarity judgements using lexical co-occurrence"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

header-includes:
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1, T2A, T5]{fontenc}
  - \usepackage{kotex}
  
author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
 Is “cow” more closely related to “grass” or “chicken”? Speakers of different languages judge similarity in this context differently, but why? One possibility is that cultures co-varying with these languages induce differences in conceptualizations of similarity. Specifically, East Asian cultures may promote reasoning about thematic similarity, by which cow and grass are more related, whereas Western cultures may bias judgements toward taxonomic relations, like cow-chicken. This difference in notions of similarity is the consensus interpretation for cross-cultural variation in this paradigm. We consider, and provide evidence for, an alternative possibility, by which notions of similarity are similar across contexts, but the statistics of the environment vary. On this account, similarity judgements are guided by co-occurrence in experience, and observing or hearing about cows and grass or cows and chickens more often could induce preferences for these groupings, and account in part for apparent differences in notions of similarity across contexts.
    
keywords: >
    similarity; culture; language; semantics; lexical co-occurrence; variation
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)
```

```{r, libraries, cache=F}
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

```{r setup, include = FALSE, cache=F}
library("papaja")
library("knitr") # for knitting things
library("tidyverse") # for all things tidyverse
library("car")
library("lme4")
library("patchwork")
library("effsize")
library("kableExtra")
library("irr")
require(agreement)

# # these options here change the formatting of how comments are rendered
# opts_chunk$set(
#   comment = "",
#   results = "hold",
#   fig.show = "hold")

# set the default ggplot theme 
theme_set(theme_classic(base_size = 7))

#r_refs("r-references.bib")
#r_refs("references/packages.bib")

```

```{r analysis-preferences, cache=F}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction 
By virtue of discrete words and grammatical features, language provides a categorical partition of our continuous experiences. By measuring the similarity between words (as commonly done with lexical co-occurrence models), it may be possible to model the shape of similarity space in order to capture cross-linguistic and cross-cultural variation and provide a more general answer to the question of how language influences our conception of the world.

Taxonomic and thematic similarity provide a convenient entry point to broader debates about cross-linguistic and cross-cultural variation in notions of similarity. Taxonomic categorization is based on the similarity of attributes, for example, similar perceptual properties, like shared color or shape, among objects. In contrast, thematic categorization is based on causal, spatial, and temporal relationships among objects [@Markman1984]. 

## Cross-cultural variation in similarity
Preferences for taxonomic and thematic similarity vary across cultures. @Chiu1972 found that Chinese children (9-10 years old) are more likely to choose thematic matches in a picture triad task (shown, e.g., images of an ant, a bee, and honey) than their American counterparts. These cross-cultural differences are also observed in novel object categorization, with Chinese participants preferring to group by family resemblance across multiple features and Americans preferring a single-feature rule [@Norenzayan2002]. 
<!-- Because differences in similarity preference persist with novel stimuli (as opposed to familiar items), this suggests that participants from different cultural backgrounds use different reasoning strategies when making similarity judgments. -->
The authors link these differences to tendencies toward analytic processing in Western cultures, which emphasizes objects and their properties, and holistic processing in East Asian cultures, which emphasizes relationships between objects and their context (see also @Nisbett2003). In related work, East Asian participants show a higher level of sensitivity to context than their Western counterparts when reproducing drawings from memory [@Ji2000]; visually exploring naturalistic scenes [@Chua2005]; describing scenes [@Masuda2001]; and in explaining the causes of ambiguous behaviors [@Choi1999]. 

@Ji2004 asked whether differences in analytic and holistic processing <!--they use that language, right?--> are driven by language, culture, or a combination of these. They presented Chinese and European American adults with a word triad task, and found a preference for thematic matching among Chinese participants compared to European Americans. They found that test language contributed to this difference, but did not fully explain it: Chinese participants showed a stronger thematic preference when tested in Mandarin, but still showed a thematic preference when tested in English. Ji et al. conclude that culture (independent of language) leads to different styles of reasoning about similarity, whereas language serves as a “tuning” mechanism operating within a culturally-specific style, by activating representations corresponding to the language being used.^[But note that this work did not include a manipulation to prevent participants from engaging in e.g., covert naming in a language other than the test language.] 

This view ascribes cross-cultural differences in similarity judgment to variation in the conceptualization of similarity itself. Alternatively, these judgments could be shaped by cross-cultural differences in the statistics of the environment, and accordingly the content of everyday experiences. Perhaps when confronted with the triad task, participants from all cultures follow the same process for reasoning about similarity, but rely on language or culture-specific inputs to this process. <!--For example, it may be that Vietnamese participants are more likely to have knowledge of farming practices than Americans, and may associate cows and grass more strongly (as grass is the typical feed for cows in the small-scale family farming contexts common in Vietnam). Americans, on the other hand, may have strong associations between cows and chickens as a result of encountering them together in children’s books or petting zoo contexts.--> If we observe a difference in categorization between East Asian and Western participants, it could be that members of both groups use the same procedure (considering similarity that is influenced by both taxonomic and thematic relations), but the inputs to this procedure differ between cultures, with East Asian participants exposed to more instances of thematic similarity than their Western counterparts.

## Estimating variation in experience
While co-occurrences in experience are difficult to measure, co-occurrence in language can provide a rough proxy--and indeed, language may afford many of the “experiences” that people have with infrequently encountered items, like cows or helicopters. In this study, we use co-occurrence in language as a proxy of cultural experience. This is a generalizing assumption, and while no linguistic corpus can be expected to fully capture a culture, using this proxy as a model of culture provides a conservative test of the hypothesis that differences in the input, rather than the process, of reasoning produces cultural variation in this task. This means that if such a model does work -- if linguistic co-occurrence can predict cultural variation in this similarity task without building differences in the decision process into the model -- it is relatively strong evidence that language alone (perhaps as a more easily observed source of information on broader cultural and ecological variation) may account for these cross-cultural differences in reasoning. 

Indeed, previous work suggests that using lexical co-occurrence as a proxy can be useful in thinking about similarity reasoning. Natural language processing tasks have found that lexical co-occurrence is a good predictor for performance in similarity judgment. @Griffiths2007 showed that a model that takes word-document co-occurrence as input can be used to predict word association and the effects of semantic association on a variety of linguistic tasks. Additionally, lexical semantic models that are constructed using lexical co-occurrence (as opposed to annotated relations) have been shown to perform well on predicting human judgments about similarity between word pairs that are thematically or taxonomically related [@Rohde2006].

Our study uses cosine distances of fastText word vectors as a measure of lexical co-occurrence^[We also carried out our analysis using raw lexical co-occurrences and obtained similar results.]. fastText is a system that uses lexical co-occurrence information to generate a vector representing each word in its lexicon [@Mikolov2018]. 

<!-- MOVE TO METHODS? fastText uses a continuous bag-of-words model, which takes into account a symmetric window of words surrounding the word in question and maximizes the log-likelihood of the probability of the word given its context. Calculating the cosine-similarity between word vectors from fastText gives us a score of how similar the contexts of these words are. Therefore, words that occur in the same context (high lexical co-occurrence) would have a larger cosine-similarity score. 

It must be noted that fastText supplements the lexical co-occurrence information by simultaneously training weight vectors for each position relative to the word in question, as well as adding sub-word information (bag-of-character n-gram vectors for each character forming the words). The similarity judgment prediction given by fastText may be more higher than a model using purely lexical co-occurrence information. However, models that use only lexical co-occurrence information (with or without a simple position weight system that gives more weight to words that are closer to the word in question) also perform well in similarity prediction tasks. For this study, due to the lack of large cleaned corpora for Mandarin and Vietnamese, we leverage fastText pre-trained models on the languages we are interested in @Grave2018.-->

<!-- condense to 2 sentences --> Systems like fastText and word2vec have been demonstrated to be good predictors for similarity judgments. @Liu2019 showed that fastText performed well as the data for a system matching responses in a knowledge base by how similar they are to a customer’s query. A study by @Jatnika2019 tested a word2vec model trained on the English wikipedia corpus against 353 word pairs labeled with similarity values based on human judgment. The word2vec model correlates moderately with human judgment. fastText has also been shown to be sensitive towards cultural effects on word meanings: @Thompson2020 applied fastText to language-specific Wikipedia corpus (among others) to generate a “semantic neighborhood” of 1010 meanings in different languages. The study showed that languages that are spoken by more similar cultures, more geographically proximate and/or historically related had more semantic neighborhoods that aligned. This is a proof of concept for our use of fastText in different languages with varying levels of relatedness.

## The present study
<!-- @Ji2004 observed a difference in similarity judgment between Chinese and European Americans (the former preferred thematic matching the latter taxonomic). However, their evidence is not sufficient to conclude that it is culture that drives this difference, as their cross-cultural data is correlational. Furthermore, they have yet to rule out all mechanisms by which language could be relevant to the task. For example, their language manipulation (bilingual speakers from Mainland China making more taxonomic categorizations when tested in language) only points to an offline effect of language because it does not prevent participants from accessing linguistic representations. A stronger claim could have been made if verbal interference had been conducted, and if cross-cultural differences had remained. This would have been stronger in eliminating online effect of language as a potential cause for cross-cultural differences in similarity judgment.  -->

<!-- Additionally, outside of language, cross-cultural differences could come from a couple different mechanisms:  -->
@Ji2004 suggest that the notion of similarity is what varies across cultural groups, but it is possible that similarity is regarded in the same way and it is actually the input to this similarity judgment that varies across cultural contexts. It may be possible to gain traction on potential mechanisms by examining whether variation in similarity judgments co-vary with environmental statistics that differ across cultural and linguistic contexts.

In this study, we ask whether variation in lexical co-occurrence can account for the cross-context (culture and language) differences observed in how people evaluate similarity. First, we attempt to replicate @Ji2004 by running a study comparing English speakers in the US and Mandarin speakers in mainland China. Second, we evaluate whether the differences observed between English and Mandarin speakers’ similarity judgments also extend to comparisons between English and Vietnamese, by running a similar study between English speakers in the US and Vietnamese speakers in Vietnam. Vietnam is a Southeast Asian country that borders China and is historically greatly influenced by Chinese culture [@Hui2002]. Therefore, it serves as a suitable cultural context to investigate whether the claim made by @Ji2004 and previous studies, that Eastern and Western cultures have different notions of similarity, extends beyond mainland China. 

We then ask to what extent similarity judgments show cross-context differences that align with co-occurrence patterns in participants’ languages. We would only see a main effect of cultural context if differing notions of similarity drive these cross-cultural differences. Meanwhile, if we do see variation in responding trial-to-trial that tracks with lexical co-occurrence statistics (as a rough proxy combining both linguistic and cultural context), this suggests that cross-cultural notions of similarity might be similar, and it is environmental statistics that guide the differences in similarity judgments. 

To preview our results, we find that, while we do not see overall group differences (between the East Asian contexts and the US) in similarity, we do find language-specific co-occurrence can indeed explain cross-context differences in similarity judgments. In contrast to previous accounts, by which culture induces differing conceptions of similarity (varying between taxonomic and thematic), these findings provide an alternative, and more specific, account by which language may explain these cross-cultural differences without invoking variation in notions of similarity. 

There are several important limitations of our approach. While we discuss cross-cultural variability at the level of countries or larger world areas, these are not cultural monoliths. For convenience, we operationalize culture at the level of country, based on where participants were raised. It is an open question whether performance in our participant populations (of relatively young and well-educated adults) is representative of the broader country. This is especially true for societies with substantial ethnic and cultural variation such as the US. We expect that our data is likely to underestimate variation both within and between the countries we sample from.

Language, culture, cognition, and individual experiences are intertwined in complex causal relationships: cultural features (farming) can lead to differences in individuals’ experiences (seeing cows and grass) and in turn these can influence language (talking more about cows and grass). But cultural features (a thematic orientation) could also more directly cause individuals to talk differently about the same experiences (mentioning what cows eat rather than what other animals cows are like). In this study, we measure language and its relation to cross-cultural differences in categorization, but these relations test only the plausibility of a language-based account; they cannot establish the direction of causality.

# Methods 
```{r get responding and demographic data, include = F}
df <- read.csv("../../../data/data_USCNVN_ENZHVI.csv")

df_main_analysis <- df
df.demog <- read.csv("../../../data/responding/data_demog.csv") %>%
  filter(subject %in% unique(df$subject))

triads_omit <- read.csv("../../../data/triads_omit.csv") %>%
  pull(x)

head(df)
```

```{r get exclusions data, include = F, cache = T}
df.excl_US <- read.csv("../../../data/responding/dataUS_excl_metadata.csv") %>%
  select(-X)
df.excl_CN <- read.csv("../../../data/responding/dataCN_excl_metadata.csv") %>%
  select(-X)
df.excl_VN <- read.csv("../../../data/responding/dataVN_excl_metadata.csv") %>%
  select(-X)
```

```{r demog_E, include = F, cache = T}
df.demog_E <- df.demog %>%
  filter(country == "US")

df.age_E <- df.demog_E %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_E <- df.demog_E %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

```{r demog_M, include = F, cache = T}
df.demog_M <- df.demog %>%
  filter(country == "CN")

df.age_M <- df.demog_M %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_M <- df.demog_M %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

```{r demog_V, include = F, cache = T}
df.demog_V <- df.demog %>%
  filter(country == "VN")

df.age_V <- df.demog_V %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_V <- df.demog_V %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

## Participants
We recruited `r df.excl_US$ppts_finished` participants from the US, `r df.excl_VN$ppts_finished` participants from Vietnam, and `r df.excl_CN$ppts_finished` participants from mainland China. US participants were recruited through snowball sampling seeded with Stanford student email lists, Vietnam participants were recruited through snowball sampling seeded with Vietnam-based student groups on Facebook, and mainland China participants were recruited through snowball sampling seeded with group chats on WeChat. US participants were compensated with $5 gift certificates (USD), VN participants received 50,000₫ (VND) in phone credit, and mainland China participants received 25CNY through WeChat credit transfer.

We excluded `r df.excl_US$ppts_finished - df.excl_US$ppts_after_att_check_excl` US participants, `r df.excl_VN$ppts_finished - df.excl_VN$ppts_after_att_check_excl` Vietnam participants, and `r df.excl_CN$ppts_finished - df.excl_CN$ppts_after_att_check_excl` China participants who did not answer all attention checks correctly. We followed 4 exclusion criteria that aim to retain only participants who are influenced by one culture: (1) non-native speakers of English and Vietnamese, respectively, (2) fluent in at least one of the other two study languages (Vietnamese for US participants, English for Vietnamese participants and Chinese participants), (3) have lived outside of the test country (US, Vietnam, or China) for more than two years, and (4) have significant international experience (more than 6 international experiences of 2 days or longer.) We did not use a particular criterion for a language if it would exclude 25% or more of any one sample. In this round of exclusion, we excluded `r df.excl_US$ppts_after_att_check_excl - df.excl_US$ppts_after_demog_excl` US, `r df.excl_VN$ppts_after_att_check_excl - df.excl_VN$ppts_after_demog_excl` Vietnam participants, and `r df.excl_CN$ppts_after_att_check_excl - df.excl_CN$ppts_after_demog_excl` China participants. After these exclusions, the US sample included `r df.excl_US$ppts_after_demog_excl` participants (`r df.gender_E %>% filter(gender == "Male") %>% pull(n)`M, `r df.gender_E %>% filter(gender == "Female") %>% pull(n)`F, `r df.gender_E %>% filter(gender == "Non-binary") %>% pull(n)` non-binary, `r df.gender_E %>% filter(gender == "Decline to answer") %>% pull(n)` other), with mean age = `r round(df.age_E$mean_age, 2)` (SD = `r round(df.age_E$sd_age, 2)`) and median age = `r df.age_E$median_age`. The Vietnam sample included `r df.excl_VN$ppts_after_demog_excl` participants (`r df.gender_V %>% filter(gender == "Nam") %>% pull(n)`M, `r df.gender_V %>% filter(gender == "Nữ") %>% pull(n)`F, `r df.gender_V %>% filter(gender == "Từ chối trả lời") %>% pull(n)` other), with mean age = `r round(df.age_V$mean_age, 2)` (SD = `r round(df.age_V$sd_age, 2)`) and median age = `r df.age_V$median_age`. The China sample included `r df.excl_CN$ppts_after_demog_excl` participants (`r df.gender_M %>% filter(gender == "男性") %>% pull(n)`M, `r df.gender_M %>% filter(gender == "女性") %>% pull(n)`F, `r df.gender_M %>% filter(gender == "拒绝回答") %>% pull(n)` other), with mean age = `r round(df.age_M$mean_age, 2)` (SD = `r round(df.age_M$sd_age, 2)`) and median age = `r df.age_M$median_age`.^[A table summarizing number of participants lost at each round of exclusions is included in the Appendix.] 

Notably, we lost a majority of our Vietnam participants (more than 60%) in the attention check exclusion. One reason we suspect why this might have happened is because our Vietnam participants are less familiar with research surveys and attention check questions, and thus might have thought too much about the attention check questions.^[We carried out an exploratory analysis where we used a less stringent attention check exclusion (covered in the final part of the Results & Analysis section). To preview our findings for this analysis, we did not find any difference in significant results when compared with the main analysis.]

## Stimuli
We adapted stimuli from previous studies to create a set of test triads consisting of a cue, with one thematic and one taxonomic match option. For example, “cow,” “grass,” and “chicken,” where “cow” is the cue, “grass” is the thematic match, and “chicken” the taxonomic match. We included 105 such triads, a superset including triads pulled from supplemental information and in-text examples across the literature, and others that we adapted or created <!--(see SI for the full list of stimuli and sources)-->. We selected triads on the basis of cultural familiarity in the US, Vietnam, and China. The triads were originally in English; they were translated to Vietnamese and Mandarin by a fluent bilingual speaker in each language. The translations were then checked for accuracy after backtranslation to English by another fluent bilingual in each language who was naive to the original English versions. -->

Each participant completed all 105 triads in sets of 21 trials at a time (10 test triads, 10 filler triads, and 1 attention check per page), by selecting the match most related to the cue (“Which thing is most closely related to the bolded item?” or "Thứ nào liên quan nhất với thứ được in đậm?"). The test triads were presented with 105 filler triads mixed in, to obscure the taxonomic-thematic two-answer forced choice structure of the test stimuli and reduce the likelihood that participants would become aware of the design. The filler triads were groups of three semantically related words, but where the match options were not distinguished by thematic vs. taxonomic similarity, e.g., cue “bird” with match options “lizard” and “toad.” Additionally, we included 10 attention check trials, which were formatted like the test and filler triads but included an instruction instead of a cue item, e.g., “Choose wife” with match options “wife” and “husband.” In total, each participant completed 210 similarity judgments and 10 attention check questions, with triads presented in randomized orders that varied between subjects. 

## Corpus model 
<!-- Our general approach is to build a model of similarity that is based on collocation counts in each language.  -->

<!-- ### Raw lexical co-occurrences -->
<!-- To give an intuition for our model, consider the cow-grass-chicken triad: we counted how many times “cow” and “grass” co-occur within a window of text in each corpus, and compare this to how many times “cow” and “chicken” co-occur. Our similarity prediction is then proportional to the relative frequency of these pairs. For example, if the thematic cow-grass match accounts for 30% of collocations for this triad (with cow-chicken making up the other 70%), then our model predicts, correspondingly, that 30% of responses to the triad will be grass, and the other 70% chicken. We then use a mixed-effects regression to evaluate how well each corpus collocate model predicts participants’ similarity judgments, across triads and languages. -->

<!-- #### Collocate retrieval and coding -->
<!-- For our English co-occurrence metric, we used the online interface of the Corpus of Contemporary American English [@COCA] to retrieve collocation counts. We recorded the raw count of times that any cue-match pair (e.g., cow-grass or cow-chicken) co-occur in a window of 19 words, which is the maximum window size in the online interface, and closest to the sentence co-occurrence metric in our VI corpus collocation counts.  -->

<!-- To determine Vietnamese co-occurrence, we used the raw frequency of sentence co-occurrences from a subset of the Vietnamese corpus in the Leipzig Corpora Collection [@VICorpus]; this corpus includes 70 million Vietnamese sentences, but our corpus data comes from a 1 million sentence subset for which co-occurrence counts are available for download. Vietnamese makes very frequent use of compositional morphology but the written language  uses spacing to delineate syllable boundaries rather than word boundaries. Accordingly, collocate searches returned instances of both target terms and many morphologically related, but distinct, words. We included in our counts any instances of the target term or close semantic neighbors containing the same morpheme(s) as long as they entailed a likely literal reference to the target term. For example, our search for collocates of the term “gà” (chicken) also returned “gà mái,” a distinct word for female chickens. Despite being a different word, “gà mái” both includes the morpheme for “chicken” and entails reference to a chicken. Accordingly, instances of both “gà” and “gà mái” were included in our collocate count for “chicken.” Some compounds do not meet this criteria, and were excluded from collocate counts. For example “trái cây” (tree.fruits) includes the same syllable as the word “cây” (tree), but refers to fruit that comes from trees, not the trees themselves, and was therefore excluded from our collocate counts for “tree.” -->

<!-- Mandarin co-occurrence was collected using the online interface of the Chinese Lexical Association Database (Lin et al, 2019). Mandarin also makes frequent use of compositional morphology. To match the Vietnamese corpus statistics, we included in our counts any instances of the target term or close semantic neighbors containing the same morpheme(s) following the rule described above. These semantic neighbors were either suggested to us by the online interface (which gave search suggestions for words beginning with the target term as we typed them in) or suggested by a native speaker of Mandarin conducting the co-occurrence count data collection. For example, in getting the collocates of the term "門" (door), we also included "門兒" (also door, but is returned as a different word). Compounds that do not meet this criteria were excluded from collocate counts. For example "門牙" (door.tooth) refers to the front tooth even though it includes the same character as the target word "門" (door), so was therefore excluded from our collocate counts for "door". -->

<!-- #### Collocate similarity model -->
<!-- From the raw co-occurrence counts of each triad, we calculated the thematic co-occurrence ratio as the number of thematic co-occurrences over the sum of thematic and taxonomic co-occurrences. We did this for both the English and Vietnamese corpus co-occurrence counts. In this way, we obtained predictions for 73 of 105 triads from the Vietnamese corpus and 104 of 105 triads from English. We therefore limited all analyses reported here to the subset of triads for which our corpus-based model can make meaningful predictions, meaning triads that have at least one non-zero collocate count (either thematic or taxonomic match). We replaced any remaining zero collocate counts with $\epsilon$ to account for sparsity in the corpus data. We then tested whether these simple relative frequency models predict US and VN responding. -->

<!-- ### Cosine distance of word vectors -->
To give an intuition for our model, consider again the cow-grass-chicken triad: we retrieved word vectors for "cow" and "grass", and calculate the cosine distance between these vectors. Similarly, we retrieved vectors for "cow" and "chicken" and calculate the cosine distance between them. Our similarity prediction is then inversely proportional to the cosine distance of these pairs. This is because a larger cosine distance means the word vectors are further apart, and thus the words are less similar. For example, if the cosine distance of thematic cow-grass is 0.7 and the cosine distance of taxonomic cow-chicken is 0.3, then our model predicts, correspondingly, that 30% of responses to the triad will be grass, and the other 70% chicken. We then use a mixed-effects regression to evaluate how well each corpus model predicts participants’ similarity judgments, across triads and languages.

#### Word vector retrieval
We use the fastText pre-trained models of English, Mandarin, and Vietnamese in @Grave2018. These models are trained on Common Crawl and Wikipedia using We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. These models were trained using a Continuous Bag of Words (CBOW) with position-weights and a window of size 5. The models use character n-grams of length 5 and 10  negative examples.From the aforementioned models, we retrieve the word vectors (dimension 300) for each word we are interested in.

#### Similarity model
From the word vectors, we calculated the cosine distance between each cue-thematic match (thematic cosine distance) and cue-taxonomic match (taxonomic cosine distance), using the spatial.distance.cosine function from the SciPy package [@2020SciPy]. We then calculated the thematic cosine distance proportion as thematic cosine distance over the sum of taxonomic cosine distance and thematic cosine distance. We did this for all three corpora. We were able to obtain predictions for all triads in all languages.

# Results

## 1. Do we extend previous work reporting a preference for taxonomic matching in the US and thematic matching in Asia?

```{r country, warning=FALSE, cache = TRUE, include=FALSE, paged.print=TRUE}
df.country <- df %>%
  group_by(subject, country) %>%
  summarize(theme_resp_percent = mean(responses_theme, na.rm = T))

df.country_sum <- df.country %>%
  group_by(country) %>%
  summarize(mean_theme_resp_percent = mean(theme_resp_percent), 
            sd_theme_resp_percent = sd(theme_resp_percent))

fit.country = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df, 
                    family = "binomial")
summary(fit.country) 
fit.country.anova = Anova(fit.country, type=3)

fit.country_EN_VN = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df %>% filter(country != "China"),
                    family = "binomial")
summary(fit.country_EN_VN) 
```
The group means of proportion of thematic response in mainland China is the highest (M = `r round((df.country_sum %>% filter(country == "China"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "China"))$sd_theme_resp_percent, 2)`), followed by the groups means in Vietnam (M = `r round((df.country_sum %>% filter(country == "Vietnam"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "Vietnam"))$sd_theme_resp_percent, 2)`), which is slightly higher than that of the US (M = `r round((df.country_sum %>% filter(country == "US"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "US"))$sd_theme_resp_percent, 2)`) (Figure 1).

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.env="figure", fig.align = "center", fig.cap="Proportion of thematic responses by country."}

#show violin plot
ggplot(df.country,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = theme_resp_percent, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_violin() +
  geom_jitter(height = 0, 
              alpha = 0.3) +  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  labs(y = "Proportion Thematic Chosen", 
       x = "Country") + 
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237")) + 
  theme(legend.position = "none")
```

To test for cross-context differences in similarity judgments between the countries, we ran a mixed-effects logistic regression predicting triad responding (taxonomic or thematic) with country (US, China, or Vietnam) as a fixed effect. As random effects, we included an intercept per subject and one per triad, as well as by-triad random slopes for country to account for variation in the country effect across triads. In R syntax, the model is: response ~ country + (1 | subject) + (country | triad).

Overall, there is a significant effect of country on proportion of thematic responses ($\chi^2$(`r fit.country.anova["country", "Df"]`) = `r round(fit.country.anova["country", "Chisq"], 2)`, p `r ifelse( fit.country.anova["country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country.anova["country", "Pr(>Chisq)"], 3)), "< .001")`). However, this effect is driven by the difference between US and China responding ($\beta$ = `r round(fixef(fit.country)["countryUS"], 2)`, p `r ifelse(as.data.frame(summary(fit.country)[["coefficients"]])["countryUS", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country)[["coefficients"]])["countryUS", "Pr(>|z|)"], 3)), "< .001")`). There is no statistical difference between the Vietnam and China responding ($\beta$ = `r round(fixef(fit.country)["countryVietnam"], 2)`, p `r ifelse(as.data.frame(summary(fit.country)[["coefficients"]])["countryVietnam", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country)[["coefficients"]])["countryVietnam", "Pr(>|z|)"], 3)), "< .001")`), and the US and Vietnam responding ($\beta$ = `r round(fixef(fit.country_EN_VN)["countryVietnam"], 2)`, p `r ifelse(as.data.frame(summary(fit.country_EN_VN)[["coefficients"]])["countryVietnam", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country_EN_VN)[["coefficients"]])["countryVietnam", "Pr(>|z|)"], 3)), "< .001")`).

On this analysis, we do not find support that the US-China tendencies toward taxonomic and thematic responding (respectively) extend to the US-Vietnam comparison. Accordingly, we cannot speak to overall biases toward thematic responding across Asian cultural contexts broadly, but we do replicate the differences documented by @Ji2004 between the US and China. However, in our corpus model comparison, we do find evidence for different, more fine-grained variation in similarity judgments between the US and Vietnam. 

## 2. Can the differences in similarity judgments between English, Mandarin and Vietnamese speakers be explained by variation in lexical statistics?
### Is responding in each cultural context predicted by lexical statistics? 
To test whether variation in lexical statistics can explain differences in similarity judgments between US and Vietnam participants, we compare logistic mixed-effects regression models fit to the thematic responding data from each country separately. We first ask how well each corpus model (English, Vietnamese, or Mandarin) predicts similarity judgments by speakers of the corresponding language (US, Vietnam, or China). To do this, we use a mixed-effects logistic regression to predict triad responses (0=taxonomic or 1=thematic) with corpus prediction (proportion of cosine distance) as a fixed effect and participant and triad as random effects.

```{r US responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
#English
fit.EN_US_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.EN_US_cos)
fit.EN_US_cos.anova = Anova(fit.EN_US_cos, type = 3)
fit.EN_US_cos.anova

#Vietnamese
fit.VI_US_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.VI_US_cos)
fit.VI_US_cos.anova = Anova(fit.VI_US_cos, type = 3)
fit.VI_US_cos.anova

#Mandarin
fit.ZH_US_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.ZH_US_cos)
fit.ZH_US_cos.anova = Anova(fit.ZH_US_cos, type = 3)
fit.ZH_US_cos.anova
```

```{r VN responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
#English
fit.EN_VN_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.EN_VN_cos)
fit.EN_VN_cos.anova = Anova(fit.EN_VN_cos, type = 3)
fit.EN_VN_cos.anova

#Vietnamese
fit.VI_VN_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.VI_VN_cos)
fit.VI_VN_cos.anova = Anova(fit.VI_VN_cos, type = 3)
fit.VI_VN_cos.anova

#Mandarin
fit.ZH_VN_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.ZH_VN_cos)
fit.ZH_VN_cos.anova = Anova(fit.ZH_VN_cos, type = 3)
fit.ZH_VN_cos.anova
```

```{r CN responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
## English
fit.EN_CN_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.EN_CN_cos)
fit.EN_CN_cos.anova = Anova(fit.EN_CN_cos, type = 3)
fit.EN_CN_cos.anova

#Vietnamese
fit.VI_CN_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.VI_CN_cos)
fit.VI_CN_cos.anova = Anova(fit.VI_CN_cos, type = 3)
fit.VI_CN_cos.anova

#Mandarin
fit.ZH_CN_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.ZH_CN_cos)
fit.ZH_CN_cos.anova = Anova(fit.ZH_CN_cos, type = 3)
fit.ZH_CN_cos.anova
```

```{r calculate range of beta statistics for cosine, warning=F, include=F}
beta.single_corpus_cos <- c(
  round(fixef(fit.EN_US_cos)["theme_cosine_prop_E"], 2),
  round(fixef(fit.VI_US_cos)["theme_cosine_prop_V"], 2), 
  round(fixef(fit.ZH_US_cos)["theme_cosine_prop_M"], 2), 
  
  round(fixef(fit.EN_CN_cos)["theme_cosine_prop_E"], 2),
  round(fixef(fit.VI_CN_cos)["theme_cosine_prop_V"], 2), 
  round(fixef(fit.ZH_CN_cos)["theme_cosine_prop_M"], 2), 
  
  round(fixef(fit.EN_VN_cos)["theme_cosine_prop_E"], 2),
  round(fixef(fit.VI_VN_cos)["theme_cosine_prop_V"], 2), 
  round(fixef(fit.ZH_VN_cos)["theme_cosine_prop_M"], 2)
)
```

We found that all corpora are significant predictors of all cultural context responding, with p<0.05 and $\beta$ from `r min(beta.single_corpus_cos)` to `r max(beta.single_corpus_cos)`. (For a full report, see Appendix.)

### Is responding in each cultural context best predicted by the corresponding corpus's lexical statistics, as opposed to the other two corpora?
Next, we directly compare the corpus models by including both as fixed effects in three mixed-effect regressions (predicting US, Vietnam and China responding) with the same random effects as above. In R syntax, the model is: response ~ corpus_English + corpus_Vietnamese + corpus_Mandarin + (1|triad) + (1|subject).

```{r Eng-Vie-Mand cosine similarities predicting US-VN-CN, warning=FALSE, include=FALSE}
df <- read.csv("../../../data/data_USCNVN_ENZHVI.csv")

## English/Vietnamese/Mandarin comparison
fit.ENVIZH_US_cos = glmer(responses_theme ~ theme_cosine_prop_E + theme_cosine_prop_V + theme_cosine_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "US"), 
                    family="binomial")
summary(fit.ENVIZH_US_cos)
fit.ENVIZH_US_cos.anova = Anova(fit.ENVIZH_US_cos, type = 3)
fit.ENVIZH_US_cos.anova

fit.ENVIZH_VN_cos = glmer(responses_theme ~ theme_cosine_prop_V + theme_cosine_prop_E + theme_cosine_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "Vietnam"), 
                    family="binomial")
summary(fit.ENVIZH_VN_cos)
fit.ENVIZH_VN_cos.anova = Anova(fit.ENVIZH_VN_cos, type = 3)
fit.ENVIZH_VN_cos.anova

fit.ENVIZH_CN_cos = glmer(responses_theme ~ theme_cosine_prop_M + theme_cosine_prop_E + theme_cosine_prop_V +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "China"), 
                    family="binomial")
summary(fit.ENVIZH_CN_cos)
fit.ENVIZH_CN_cos.anova = Anova(fit.ENVIZH_CN_cos, type = 3)
fit.ENVIZH_CN_cos.anova
```

For US responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

For Vietnam responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

For China responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

We observed some level of language specificity from this analysis. The English corpus is the best predictor for US responding, and the Mandarin corpus is the best predictor for China response. While this is not the case with the Vietnamese corpus and the Vietnam responding, the Vietnamese corpus is still a significant predictor for the Vietnam responding (Figure 2).

```{r echo=FALSE, warning = FALSE}
coeff_value_cos <- c(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_V"], 
                 fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_M"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_V"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_M"], 
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_V"], 
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_M"])

se_value_cos <- c(summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_M", "Std. Error"], 
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_M", "Std. Error"],
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_M", "Std. Error"])

country_value <- c(rep("US", 3),
                   rep("VN", 3),
                   rep("CN", 3))

corpus_value <- c(rep(c("EN", "VI", "ZH"), 3))

df.USVNCN_coeffs_cos <- data.frame(country_value, corpus_value, coeff_value_cos, se_value_cos) %>%
  mutate(corpus_value = factor(corpus_value, levels=c("ZH", "EN", "VI")))

plot.coeffs_cos <- ggplot(data = df.USVNCN_coeffs_cos, 
       mapping = aes(x = country_value, y = coeff_value_cos, fill = corpus_value)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin= coeff_value_cos - se_value_cos, ymax = coeff_value_cos + se_value_cos), width=.2,
                 position=position_dodge(.9)) +
  scale_y_reverse() + 
  labs(x = "Country", y = "Fixed effect size", fill = "Corpus") + 
  scale_x_discrete(labels=c("CN" = "China", "US" = "US", "VN" = "Vietnam")) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237"))
```

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.env = "figure", fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap="Fixed effect sizes of each corpus lexical statistics (cosine distance proportion) when included as a predictor for China, US, and Vietnam responding, respectively. The English corpus is the best predictor for US response, and the Mandarin corpus is the best predictor for China response."}
#plot.coeffs_freq + 
  plot.coeffs_cos
```

```{r model comparison, include=FALSE}
fit.US_cos_compare = anova(fit.ENVIZH_US_cos, fit.EN_US_cos, type = 3)
fit.VN_cos_compare = anova(fit.ENVIZH_VN_cos, fit.VI_VN_cos, type = 3)
fit.CN_cos_compare = anova(fit.ENVIZH_CN_cos, fit.ZH_CN_cos, type = 3)
```

However, we found that language specificity alone does not explain our results. We used an ANOVA to compare the model with only the corresponding corpus, and the model with all 3 corpora for each cultural context. We found that in all three cases (US, China, Vietnam), adding the other two corpora produces a significantly better fit than the identical model without the additional corpora, and only the corresponding corpus included as a predictor (US response: $\chi^2$(`r fit.US_cos_compare["fit.ENVIZH_US_cos", "Df"]`) = `r round(fit.US_cos_compare["fit.ENVIZH_US_cos", "Chisq"], 2)`, p `r ifelse( fit.US_cos_compare["fit.ENVIZH_US_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.US_cos_compare["fit.ENVIZH_US_cos", "Pr(>Chisq)"], 3)), "< .001")`; Vietnam response: $\chi^2$(`r fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Df"]`) = `r round(fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Chisq"], 2)`, p `r ifelse( fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"], 3)), "< .001")`; China response: $\chi^2$(`r fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Df"]`) = `r round(fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Chisq"], 2)`, p `r ifelse( fit.CN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.CN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"], 3)), "< .001")`). 

## 3. Does similarity reasoning differ across cultures, only the input to it, or both?
Our analysis shows that lexical statistics of each corpus is a significant predictor for similarity judgement in the corresponding cultural context, even without the addition of the other two corpora. Assuming that lexical statistics is a good proxy for environmental statistics, we take the above result as evidence that cross-cultural differences in input to the process of similarity judgement at least partially drives cross-cultural similarity judgement differences. 
However, it is still an open question whether the nature of similarity judgement (for e.g., the notion of similarity) itself differs across cultures and contributes to cross-cultural differences in similarity judgement responding. In an alternative hypothesis, the cross-cultural differences in similarity judgement observed is driven by not only the input, but also an interaction between cultural-specific nature of similarity judgement and input. To test this hypothesis, we operationalize cultural-specific nature of similarity judgement as country context, and input to similarity judgement as the corresponding corpus statistics. We then compare a model that includes country, corresponding corpus statistics and their interaction as fixed effects (R syntax: responses ~ corresponding_corpus * country + (1|triad) + (1|subject)) to a model with only the corresponding corpus statistics as the fixed effect (R syntax: responses ~ corresponding_corpus + (1|triad) + (1|subject))

```{r add corresponding language, include = F}
df <- df %>%
  mutate(theme_freq_prop_corr_lang = case_when(
    country == "US" ~ theme_freq_prop_E,
    country == "China" ~ theme_freq_prop_M, 
    country == "Vietnam" ~ theme_freq_prop_V)) %>% 
    mutate(theme_cosine_prop_corr_lang = case_when(
    country == "US" ~ theme_cosine_prop_E,
    country == "China" ~ theme_cosine_prop_M, 
    country == "Vietnam" ~ theme_cosine_prop_V))
```

<!-- ```{r country language interaction in raw co-occurrences, include = F} -->
<!-- fit.language = glmer(responses_theme ~ theme_freq_prop_corr_lang + (1 | subject) + (1 | triad),  -->
<!--           data = df %>% filter(!(triad %in% triads_omit)),   -->
<!--           family="binomial") -->
<!-- summary(fit.language) -->
<!-- fit.language.anova = Anova(fit.language, type = 3) -->
<!-- fit.language.anova -->

<!-- fit.country_language = glmer(responses_theme ~ theme_freq_prop_corr_lang * country + (1 | subject) + (1 | triad),  -->
<!--           data = df %>% filter(!(triad %in% triads_omit)),   -->
<!--           family="binomial") -->
<!-- summary(fit.country_language) -->
<!-- fit.country_language.anova = Anova(fit.country_language, type = 3) -->
<!-- fit.country_language.anova -->

<!-- fit.country_language_compare = anova(fit.language, fit.country_language, type = 3) -->
<!-- fit.country_language_compare -->
<!-- ``` -->

```{r country language interaction in cosine prop, include = F}
fit.language_cos = glmer(responses_theme ~ theme_cosine_prop_corr_lang + (1 | subject) + (1 | triad), 
          data = df,  
          family="binomial")
summary(fit.language_cos)
fit.language_cos.anova = Anova(fit.language_cos, type = 3)
fit.language_cos.anova

fit.country_language_cos= glmer(responses_theme ~ theme_cosine_prop_corr_lang * country + (1 | subject) + (1 | triad), 
          data = df,  
          family="binomial")
summary(fit.country_language_cos)
fit.country_language_cos.anova = Anova(fit.country_language_cos, type = 3)
fit.country_language_cos.anova

fit.country_language_cos_compare = anova(fit.language_cos, fit.country_language_cos, type = 3)
fit.country_language_cos_compare
```
In the model containing only the corresponding corpus statistics, the corpus statistics is a significant predictor ($\beta$ = `r round(fixef(fit.language_cos)["theme_cosine_prop_corr_lang"], 2)`, $\chi^2$(`r fit.language_cos.anova["theme_cosine_prop_corr_lang", "Df"]`) = `r round(fit.language_cos.anova["theme_cosine_prop_corr_lang", "Chisq"], 2)`, p `r ifelse( fit.language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"], 3)), "< .001")`). When adding country and interaction between country and corpus, both the corpus statistics ($\beta$ = `r round(fixef(fit.country_language_cos)["theme_cosine_prop_corr_lang"], 2)`, $\chi^2$(`r fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Df"]`) = `r round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Chisq"], 2)`, p `r ifelse( fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"], 3)), "< .001")`) and interaction between country and corpus ($\beta$ = `r round(fixef(fit.country_language_cos)["theme_cosine_prop_corr_lang:country"], 2)`, $\chi^2$(`r fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Df"]`) = `r round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Chisq"], 2)`, p `r ifelse( fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Pr(>Chisq)"], 3)), "< .001")`) are significant predictors. This suggests that there are cross-cultural differences in both input and nature of similarity judgement that drive differences in similarity judgement responding.

## 4. How general are the cross-language differences in similarity judgments we observe? Are they limited to taxonomic-thematic contrasts or do they extend to other cases?
Previous work using the triad task such as @Ji2004 has focused on contrasting taxonomic-thematic match preferences, driven by a theory of cross-cultural differences in notions of similarity. Meanwhile, our approach has shown that notions of similarity only has an effect on cross-cultural differences in similarity judgement when modulated by input, and that input as proxied by lexical statistics of the corresponding corpus by itself is a significant predictor for similarity judgement responding. 
Our approach using lexical statistics do not assume a taxonomic/thematic structure in the triads. Therefore, it should be possible to predict similarity judgements to triads that do not follow a taxonomic/thematic structure, such as our filler triads. If our approach is tractable, we should be able to repeat the above analyses to the filler triads, and find that lexical statistics of each corpus is a significant predictor for similarity judgements in filler triads of the corresponding cultural context.

```{r incorporate base freq for fillers}
df.filler <- read.csv("../../../data/data_filler_USCNVN_ENZHVI.csv")
fillers_omit <- read.csv("../../../data/fillers_omit.csv") %>%
  pull(x)
```

```{r country summary filler, warning=FALSE, cache = TRUE, include = FALSE, paged.print=TRUE}
df.country_filler <- df.filler %>%
  group_by(subject, country) %>%
  summarize(word1_resp_percent = mean(responses_word1, na.rm = T))

df.country_sum_filler <- df.country_filler %>%
  group_by(country) %>%
  summarize(mean_word1_resp_percent = mean(word1_resp_percent), 
            sd_word1_resp_percent = sd(word1_resp_percent))
``` 

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.env="figure", fig.align = "center", fig.cap="Proportion of thematic responses by country."}
#show violin plot
ggplot(df.country_filler,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = word1_resp_percent, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_violin() +
  geom_jitter(height = 0, 
              alpha = 0.3) +  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  labs(y = "Proportion Word1 Chosen", 
       x = "Country") + 
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237")) + 
  theme(legend.position = "none")
```

```{r warning=FALSE, include = F, cache = TRUE, fig.env="figure", paged.print=TRUE}
# df.country_sum <- df.country %>%
#   group_by(country) %>%
#   summarize(mean_theme_resp_percent = mean(theme_resp_percent), 
#             sd_theme_resp_percent = sd(theme_resp_percent))

fit.country_filler = glmer(responses_word1 ~ country + (1 | subject) + (country | cue),
                    data = df.filler, 
                    family = "binomial")
summary(fit.country_filler) 
fit.country_filler.anova = Anova(fit.country_filler, type=3)
fit.country_filler.anova
``` 
For each filler triad, we randomly assigned one of the responding optiosn as 'Word1'. Running a mixed-effects logistic regression predict responding (Word1 or Word2) with structure equivalent to Analysis 1, we found no effect of country on filler responding (Figure 3).

### Does lexical statistics for fillers predict responding in the corresponding cultural context?
<!-- #### Raw co-occurrences -->
<!-- ```{r US responding predicted by each corpus model raw co-occurrences for fillers, warning=FALSE,include = F} -->
<!-- # English  -->
<!-- fit.EN_US_filler = glmer(responses_word1 ~ word1_match_frequency_prop_E + (1 | subject) + (1 | cue),  -->
<!--           data = df.filler %>% filter(country == "US",  -->
<!--                                !(cue_id %in% fillers_omit)),  -->
<!--           family="binomial") -->
<!-- summary(fit.EN_US_filler) -->
<!-- fit.EN_US_filler.anova = Anova(fit.EN_US_filler, type = 3) -->

<!-- fit.EN_US_filler.anova -->
<!-- ``` -->

<!-- ```{r VN responding predicted by each corpus model raw co-occurrences for fillers, warning=FALSE, include = F} -->
<!-- # Vietnamese -->
<!-- fit.VI_VN_filler = glmer(responses_word1 ~ word1_match_frequency_prop_V + (1 | subject) + (1 | cue),  -->
<!--           data = df.filler %>% filter(country == "Vietnam",  -->
<!--                                !(cue_id %in% fillers_omit)),  -->
<!--           family="binomial") -->
<!-- summary(fit.VI_VN_filler) -->
<!-- fit.VI_VN_filler.anova = Anova(fit.VI_VN_filler, type = 3) -->

<!-- fit.VI_VN_filler.anova -->
<!-- ``` -->
<!-- Yes, corpus raw co-occurrences predicts responding for both US and Vietnamese. *need to discuss with Shan to incorporate Mandarin due to a lot of empty counts* -->

<!-- #### Cosine distance -->
```{r US responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# English 
fit.EN_US_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_E + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "US"), 
          family="binomial")
summary(fit.EN_US_filler_cos)
fit.EN_US_filler_cos.anova = Anova(fit.EN_US_filler_cos, type = 3)

fit.EN_US_filler_cos.anova
```

```{r VN responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# Vietnamese
fit.VI_VN_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_V + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.VI_VN_filler_cos)
fit.VI_VN_filler_cos.anova = Anova(fit.VI_VN_filler_cos, type = 3)

fit.VI_VN_filler_cos.anova
```

```{r CN responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# Vietnamese
fit.ZH_CN_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_M + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "China"), 
          family="binomial")
summary(fit.ZH_CN_filler_cos)
fit.ZH_CN_filler_cos.anova = Anova(fit.ZH_CN_filler_cos, type = 3)

fit.ZH_CN_filler_cos.anova
```

```{r calculate range of beta statistics for cosine fillers, warning=F, include=F}
beta.filler_single_corpus_cos <- c(
  round(fixef(fit.EN_US_filler_cos)["word1_match_cosine_prop_E"], 2),
  round(fixef(fit.ZH_CN_filler_cos)["word1_match_cosine_prop_M"], 2), 
  round(fixef(fit.VI_VN_filler_cos)["word1_match_cosine_prop_V"], 2)
)
```
Using the same mixed-effects logistic regression structure as Analysis 2, we predict responses (1=word1 or 0=word2) with corpus prediction (proportion of cosine distance) as a fixed effect and participant and triad as random effects. We found that in all cultural contexts, the correpsonding corpora is a significant predictor of responding, with p<0.05 and $\beta$ from `r min(beta.filler_single_corpus_cos)` to `r max(beta.filler_single_corpus_cos)`. (For a full report, see Appendix.)

# Discussion
In this paper, we consider whether statistics of the language environment can account for cross-cultural differences in a classic similarity judgment paradigm, as an alternative to the view that members of different cultures vary in their conception of similarity. 

We first tested the generality of a cultural account which holds that people from Western and East Asian cultures tend to conceive of similarity in more taxonomic and thematic ways, respectively, and respond accordingly in categorization tasks such as ours. While we managed to replicate the previously documented contrast between English speakers in the US and Mandarin Chinese speakers from East Asia (mainland China, Taiwan, Hong Kong, and Singapore), we do not extend this contrast to our sample of Vietnamese speakers in Vietnam and English speakers in the US. This finding suggests some limitations on the generality of this cultural account. 

We did find some signatures of language specificity in our analysis, such as the large positive correlation between similarity judgments of each country and the respective corpus statistics, and how each corpus statistics are good predictors for corresponding country's similarity judgments. However, this is potentially due to the high correlation between corpus statistics of English, Vietnamese and Mandarin. We find even stronger evidence for consistency across the three groups, with substantive overlapping predictions across the corpus models, highly similar responding across the experiments, and a correspondingly high fit in cross-language comparisons between models and data.

Our findings raise additional questions for future work: if not differences in taxonomic vs. thematic responding, then what differences drive the relativity effects previous studies have observed? To what extent are the relativity effects driven by language, and to what extent by culture? @Ji2004 established that culture-aligned differences in this paradigm exist, even when the test language is held constant, concluding that “it is culture (independent of the testing language) that led to different grouping styles” in their study. Our data provide a cautionary note to this conclusion, suggesting that semantic representations in bilinguals (see @Francis2005 for a review) may have the potential to provide an offline account for cross-context differences in similarity judgments, independent of test language. However, there are still many open questions for this account. How do semantic associations guide categorization? Can they explain taxonomic-thematic differences of the type reported by @Ji2004 and others? Can we provide a more specific computational account than the simple frequency model tested here?

Despite these caveats, our findings here demonstrate the plausibility of an alternative perspective on cross-cultural accounts of language, thought, and similarity in the case of taxonomic and thematic reasoning: that it may be the input to similarity judgments, rather than the evaluative process or the conceptualization of similarity that produces variation in similarity reasoning across cultural and linguistic contexts. We hope this work provides a foundation for further research probing this question. 

# Appendix
## 1. ICC of item variability and participant variability 

```{r xtable, results ="asis", fig.pos="H"}
US_agr <- agreement::dim_icc(df_main_analysis %>% filter(country == "US"),
                             model = "2A",
                             type = "agreement",
                             unit = "average",
                             object = triad,
                             rater = subject,
                             score = responses_theme,
                             bootstrap = 0)

CN_agr <- agreement::dim_icc(df_main_analysis %>% filter(country == "China"),
                             model = "2A",
                             type = "agreement",
                             unit = "average",
                             object = triad,
                             rater = subject,
                             score = responses_theme,
                             bootstrap = 0)

VN_agr <- agreement::dim_icc(df_main_analysis %>% filter(country == "Vietnam"),
                             model = "2A",
                             type = "agreement",
                             unit = "average",
                             object = triad,
                             rater = subject,
                             score = responses_theme,
                             bootstrap = 0)

iccs <- tibble(Sample = c("US","China", "Vietnam"),
               `Intra-rater ICC` = c(US_agr$Intra_ICC,
                                     CN_agr$Intra_ICC,
                                     VN_agr$Intra_ICC),
               `Inter-rater ICC` = c(US_agr$Inter_ICC,
                                     CN_agr$Inter_ICC,
                                     VN_agr$Inter_ICC))

tab1 <- xtable::xtable(iccs, digits=2,
                       caption = "ICCs for reliability within participants (intra-rater) and stimuli (inter-rater) for all cultural contexts.")

print(tab1, type="latex", comment = F, include.rownames = FALSE)
```

## 2. Supplemental for whether each corpus predicts similarity judgement in each cultural context

### Triads
For US responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are significant predictors of US responding (EN-US model: $\beta$ = `r round(fixef(fit.EN_US_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_US_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.EN_US_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`; VI-US model: $\beta$ = `r round(fixef(fit.VI_US_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_US_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.VI_US_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`; ZH-US model: $\beta$ = `r round(fixef(fit.ZH_US_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_US_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ZH_US_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`).

For Vietnam (VN) responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are significant predictors of VN responding (EN-VN model: $\beta$ = `r round(fixef(fit.EN_VN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_VN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.EN_VN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`; VI-VN model: $\beta$ = `r round(fixef(fit.VI_VN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_VN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.VI_VN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`; ZH-VN model: $\beta$ = `r round(fixef(fit.ZH_VN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`).

For China (CN) responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are significant predictors of CN responding (EN-CN model: $\beta$ = `r round(fixef(fit.EN_CN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_CN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.EN_CN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`; VI-CN model: $\beta$ = `r round(fixef(fit.VI_CN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_CN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.VI_CN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`; ZH-CN model: $\beta$ = `r round(fixef(fit.ZH_CN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`).

### Fillers
English (EN) is a significant predictors of US responding ($\beta$ = `r round(fixef(fit.EN_US_filler_cos)["word1_match_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_US_filler_cos.anova["word1_match_cosine_prop_E", "Df"]`) = `r round(fit.EN_US_filler_cos.anova["word1_match_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_US_filler_cos.anova["word1_match_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_US_filler_cos.anova["word1_match_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`).

Vietnamese (VI) is a significant predictors of Vietnam responding ($\beta$ = `r round(fixef(fit.VI_VN_filler_cos)["word1_match_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_VN_filler_cos.anova["word1_match_cosine_prop_V", "Df"]`) = `r round(fit.VI_VN_filler_cos.anova["word1_match_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_VN_filler_cos.anova["word1_match_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_VN_filler_cos.anova["word1_match_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`).

Mandarin (ZH) is a significant predictors of China responding ($\beta$ = `r round(fixef(fit.ZH_CN_filler_cos)["word1_match_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_CN_filler_cos.anova["word1_match_cosine_prop_M", "Df"]`) = `r round(fit.ZH_CN_filler_cos.anova["word1_match_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_CN_filler_cos.anova["word1_match_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_CN_filler_cos.anova["word1_match_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`).

## 3. Demographic Exclusion Statistics
```{r results = "asis", fig.cap="Testing"}
df.demog_excl <- cbind(t(df.excl_US), t(df.excl_CN))
df.demog_excl <- cbind(df.demog_excl, t(df.excl_VN))

rownames(df.demog_excl) <-
  c("Country", "Ppts Finished", 
    "Excluded", 
    "After Exclusion", 
    "Non-Native Speakers", "EN Speakers", 
    "VI Speakers", "ZH Speakers", 
    "Lived Abroad", "Overseas Experience", 
    "After Exclusions")

colnames(df.demog_excl) <- df.demog_excl[1,]
df.demog_excl <- df.demog_excl[-1, ] 

kbl(df.demog_excl, align = "c") %>%
  add_header_above(c(" " = 1, "Country" = 3)) %>%
  pack_rows(index = c(" " = 1, "Attention Check Exclusions" = 2, "Demographics Exclusions" = 7))
```

\newpage

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

\newpage
