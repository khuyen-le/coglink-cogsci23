---
title: "Re-examining cross-cultural similarity judgments using lexical co-occurrence"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
header-includes:
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1, T2A, T5]{fontenc}
  - \usepackage{kotex}
author-information: > 
    \author{{\large \bf Khuyen N. Le (knl005@ucsd.edu)}$^{1}$, {\large \bf Alexandra Carstensen (abc@ucsd.edu)}$^{1}$, \\ {\large \bf Shan Gao (shangaocog@gmail.com)$^2$}, {\large \bf Michael C. Frank (mcfrank@stanford.edu)$^3$},  \AND
    $^1$Department of Psychology, University of California, San Diego, $^2$Department of Psychology, University of Chicago, \\ $^3$Department of Psychology, Stanford University
    }
abstract: >
 Is “cow” more closely related to “grass” or “chicken”? Speakers of different languages judge similarity in this context differently, but why? One possibility is that cultures co-varying with these languages induce differences in conceptualizations of similarity. Specifically, East Asian cultures may promote reasoning about thematic similarity, by which cow and grass are more related, whereas Western cultures may bias judgments toward taxonomic relations, like cow-chicken. This difference in notions of similarity is the consensus interpretation for cross-cultural variation in this paradigm. We consider, and provide evidence for, an alternative possibility, by which notions of similarity are similar across contexts, but the statistics of the environment vary. On this account, similarity judgments are guided by co-occurrence in experience, and hearing about cows and grass or cows and chickens more often could induce preferences for these groupings, and account in part for apparent differences in notions of similarity across contexts.
    
keywords: >
    similarity; culture; language; semantics; lexical co-occurrence; variation; US; China; Vietnam
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
editor_options: 
  markdown: 
    wrap: sentence
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)
```

```{r, libraries, cache=F}
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

```{r setup, include = FALSE, cache=F}
library("papaja")
library("knitr") # for knitting things
library("tidyverse") # for all things tidyverse
library("car")
library("lme4")
library("emmeans")
library("patchwork")
library("effsize")
library("kableExtra")
library("irr")
require(agreement)
library("report")
library("MuMIn")

# # these options here change the formatting of how comments are rendered
# opts_chunk$set(
#   comment = "",
#   results = "hold",
#   fig.show = "hold")

# set the default ggplot theme 
theme_set(theme_classic(base_size = 7))

#r_refs("r-references.bib")
#r_refs("references/packages.bib")

```

```{r analysis-preferences, cache=F}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

<!-- By virtue of discrete words and grammatical features, language provides a categorical partition of our continuous experiences. -->

<!-- By measuring the similarity between words (as commonly done with lexical co-occurrence models), it may be possible to model the shape of similarity space in order to capture cross-linguistic and cross-cultural variation and provide a more general answer to the question of how language influences our conception of the world. -->

<!-- Taxonomic and thematic similarity provide a convenient entry point to broader debates about cross-linguistic and cross-cultural variation in notions of similarity. -->

<!-- Taxonomic categorization is based on the similarity of attributes, for example, similar perceptual properties, like shared color or shape, among objects. -->

<!-- In contrast, thematic categorization is based on causal, spatial, and temporal relationships among objects [@Markman1984]. -->

<!-- We constantly partition our continuous experience of the world into categories, but the way we organize these partitions can vary across cultures.-->

Many cognitive processes rely on similarity, from inference and generalization to analogy, mathematics, and science. There is substantial consistency in human similarity reasoning, but also systematic variation across cultural and linguistic contexts. In particular, there is considerable evidence showing that similarity reasoning in East Asian cultural contexts differs from that in Western cultures (e.g. @Nisbett2003b).
<!-- Khuyen could you add this ref as Nisbett2003b?  https://www.pnas.org/doi/10.1073/pnas.1934527100-->
For example, in a triad task comparing preferences for taxonomic and thematic similarity (choose two out of three words that are most related to one another), @Ji2004 found that Chinese participants preferred thematic matching to a greater extent than European Americans.
In an image version of this task, Chinese children (9-10 years old) are also more likely to choose thematic matches compared to their American counterparts [@Chiu1972].
This cross-cultural difference is also observed in novel object categorization, with Chinese participants preferring to group by family resemblance across multiple features and Americans preferring a single-feature rule [@Norenzayan2002].
Across tasks, East Asian participants show a preference for thematic similarity based on causal, spatial, and temporal relationships while Western participants are more likely to make taxonomic matches based on the similarity of attributes, like shared color or shape, among objects [see @Markman1984 for a discussion of these types of similarity].

An influential perspective within cultural psychology links these differences in similarity judgment to tendencies toward analytic processing in Western cultures, and holistic processing in East Asian cultures [@Nisbett2003].
Analytic processing emphasizes rule-like relationships predicated on objects and their properties and, correspondingly, taxonomic similarity, while holistic processing emphasizes relations between objects and their context and therefor, thematic similarity.
In related work, East Asian participants show a higher level of sensitivity to context than their Western counterparts when reproducing drawings from memory [@Ji2000]; visually exploring naturalistic scenes [@Chua2005]; describing scenes [@Masuda2001]; and in explaining the causes of ambiguous behaviors [@Choi1999]. The consensus interpretation in this literature ascribes cross-cultural differences in similarity judgment to variation in the *conceptualization* of similarity -- with people from East Asian cultures relying on a more thematic notion of similarity than Westerners.

Alternatively, these judgments could be shaped by cross-cultural differences in the *input* to similarity judgment, that is, the statistics of the environment, and the content of everyday experiences.
Perhaps when faced with the triad task, participants from all cultures follow the same process for conceptualizing similarity, but rely on language or culture-specific input to this process.
If we observe a difference in categorization between East Asian and Western participants, it could be that members of both groups use the same procedure (considering similarity that is influenced by both taxonomic and thematic relations), but the input to this procedure differ between cultures, with East Asian participants exposed to more support for thematic similarity in the language they hear in comparison to their Western counterparts.
We might also expect that both conceptualization of similarity and input from experience play a role in driving cross-cultural differences -- East Asian participants may be exposed to more instances of thematic similarity, and prefer to conceptualize similarity as thematic to a greater extent than Westerners.

## Estimating variation in experience via language statistics

To determine whether varied input to similarity judgments can in part explain cross-cultural differences, we need a way to operationalize variation in exposure to thematic and taxonomic relations.
While exposure to types of similarity is difficult to measure, the statistics of language can provide a rough proxy.
Language statistics are useful in that they are part of the input to everyday experience -- and indeed, may afford many of the 'experiences' that people have with infrequently encountered items, like cows or helicopters -- and they provide an accessible measure.
Previous work also suggests that language statistics, such as lexical co-occurrence or cosine distance of word embeddings, can be good predictors of similarity reasoning.
Semantic models that are constructed using lexical co-occurrence (in comparison to annotated relations) have been shown to perform well on predicting human judgments about similarity between word pairs that are thematically or taxonomically related [@Rohde2006]. 
Relatedly, a model trained on word-document co-occurrence can predict word association and the effects of semantic association on a variety of linguistic tasks [@Griffiths2007].
Word embeddings like word2vec, gloVe, and fastText have also been shown to be good predictors for similarity judgments (@Liu2019, @Jatnika2019).

Our study uses cosine distances of fastText word vectors as a measure of lexical co-occurrence[^2].
fastText is a system that uses lexical co-occurrence information to generate a vector representing each word in its lexicon [@Mikolov2018].
fastText has also been shown to be sensitive to cultural differences in word meanings: @Thompson2020 demonstrated that the distribution of semantic meaning clusters generated by fastText trained on language-specific corpora correlates with the cultural, historical, and geographical similarities of these languages.
<!-- This proof of concept provides support for our use of fastText in comparing different languages with varying levels of relatedness.-->

[^2]: We also carried out our analysis using raw lexical co-occurrences and obtained similar results.

We note that language statistics may incorporate both cultural-specific environmental statistics, that is, the experiential *input* to similarity judgments, and culture-specific senses of similarity, the *conceptualization* of similarity.
For example, cultural features (like farming) can lead to differences in environmental statistics (seeing cows and grass) and these can influence language (talking more about cows and grass).
But other cultural features (like conceiving of similarity thematically) could also cause individuals to talk differently about the same experiences (mentioning what cows eat rather than what other animals cows are like).
Acoordingly, our approach examines the extent to which language statistics can predict cross-cultural differences in similarity judgments with the understanding that language statistics are likely a proxy for both *input* to and *conceptualization* of similarity.

## The present study

<!-- @Ji2004 observed a difference in similarity judgment between Chinese and European Americans (the former preferred thematic matching the latter taxonomic). However, their evidence is not sufficient to conclude that it is culture that drives this difference, as their cross-cultural data is correlational. Furthermore, they have yet to rule out all mechanisms by which language could be relevant to the task. For example, their language manipulation (bilingual speakers from Mainland China making more taxonomic categorizations when tested in language) only points to an offline effect of language because it does not prevent participants from accessing linguistic representations. A stronger claim could have been made if verbal interference had been conducted, and if cross-cultural differences had remained. This would have been stronger in eliminating online effect of language as a potential cause for cross-cultural differences in similarity judgment.  -->

<!-- Additionally, outside of language, cross-cultural differences could come from a couple different mechanisms:  -->

```{=html}
<!--@Ji2004 suggest that the notion of similarity is what varies across cultural groups, but it is possible that similarity is regarded in the same way and it is actually the input to this similarity judgment that varies across cultural contexts.
It may be possible to gain traction on potential mechanisms by examining whether variation in similarity judgments co-vary with environmental statistics that differ across cultural and linguistic contexts.--
```
The present study tests whether the statistics of language can be used to predict cross-cultural differences in similarity judgments, and particularly, whether this approach provides additional insight beyond cross-cultural characterizations based on taxonomic vs thematic preferences.

We measured taxonomic versus thematic similarity matching in a forced-choice word triad task in three populations. Following @Ji2004, we measured preferences in the US and China. In addition, we collected data in a novel context: Vietnam. Vietnam is a Southeast Asian country that borders China and has historically been greatly influenced by Chinese culture [@Hui2002].
Therefore, it serves as a suitable cultural context to investigate whether the claim made by @Ji2004 and previous studies -- that Eastern and Western cultures have different notions of similarity -- extends beyond mainland China. In addition to these replication and extension questions, we tested whether fastText vectors from corpora corresponding to each language context (English, Mandarin, and Vietnamese) are good predictors for similarity judgments in each population.

This study is correlational and cannot evaluate causal relationships between environmental statistics and similarity judgments. However, this work can inform potential mechanisms by examining whether similarity judgments covary with environmental statistics that differ across these contexts.
Our specific research questions are as follows: 

1. Do we replicate cross-cultural differences in similarity judgments between East Asian and Western cultures?

2. Are these cross-cultural differences related to differences in language?

3. Is our language model specific to items with a taxonomic-thematic contrast, or can it predict similarity more generally? 

To preview our results, we replicate US-China differences and find that Vietnamese judgments are intermediate between these two. We find that language-specific statistics provide good predictions for cross-cultural differences in similarity judgments, and additionally, for more general similarity judgments that do not contrast taxonomic and thematic matches. Critically, the stimuli used to assess this more general case of similarity reasoning were constructed to be outside the scope of explanation for taxonomic-thematic accounts (for use as filler items). While the language model may succeed in taxonomic-thematic similarity predictions by picking up on differences in the *conceptualization* of similarity that are reflected in language data, this conceptualization account provides no prediction for these filler stimuli. Accordingly, these filler stimuli provide the strongest test of the language model, demonstrating that it can explain variation in similarity judgments beyond that explained by thematic or taxonomic conceptualizations of similarity. Taken together, our findings provide support for an alternative to previous accounts, on which differing cultures induce differing conceptions of similarity. They show that language can account for culture-specific variation in similarity reasoning, both for taxonomic-thematic judgments and for judgments not attributable to these senses of similarity. While these findings do not rule out contributions from differing senses of similarity, they show it is possible to explain cross-cultural differences in similarity judgments without invoking variation in notions of similarity, at least in some cases.

# Methods

```{r get responding and demographic data, include = F}
#df <- read.csv("../../../data/data_USCNVN_ENZHVI.csv")
df <- read.csv("../../../data/data_USCNVN_ENZHVI_exploratory.csv")

df_main_analysis <- df
df.demog <- read.csv("../../../data/responding/data_demog.csv") %>%
  filter(subject %in% unique(df$subject)) %>%
  select(-X)

triads_omit <- read.csv("../../../data/triads_omit.csv") %>%
  pull(x)

head(df)
```

```{r get exclusions data, include = F, cache = T}
df.excl_US <- read.csv("../../../data/responding/exploratory/dataUS_excl_metadata.csv") %>%
  select(-X)
df.excl_CN <- read.csv("../../../data/responding/exploratory/dataCN_excl_metadata.csv") %>%
  select(-X)
df.excl_VN <- read.csv("../../../data/responding/exploratory/dataVN_excl_metadata.csv") %>%
  select(-X)
```

```{r demog_E, include = F, cache = T}
df.demog_E <- df.demog %>%
  filter(country == "US") %>%
  filter(subject %in% unique(df$subject))

df.age_E <- df.demog_E %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_E <- df.demog_E %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

```{r demog_M, include = F, cache = T}
df.demog_M <- df.demog %>%
  filter(country == "CN") %>%
  filter(subject %in% unique(df$subject))

df.age_M <- df.demog_M %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_M <- df.demog_M %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

```{r demog_V, include = F, cache = T}
df.demog_V <- df.demog %>%
  filter(country == "VN") %>%
  filter(subject %in% unique(df$subject))

df.age_V <- df.demog_V %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_V <- df.demog_V %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

## Participants

We recruited `r df.excl_US$ppts_finished` participants from the US, `r df.excl_VN$ppts_finished` participants from Vietnam, and `r df.excl_CN$ppts_finished` participants from mainland China.
US participants were recruited through snowball sampling seeded with Stanford student email lists, Vietnam participants were recruited through snowball sampling seeded with Vietnam-based student groups on Facebook, and mainland China participants were recruited through snowball sampling seeded with group chats on WeChat.
US participants were compensated with \$5 gift certificates (USD), VN participants received 50,000₫ (VND) in phone credit, and mainland China participants received 25CNY through WeChat credit transfer.

We excluded `r df.excl_US$ppts_finished - df.excl_US$ppts_after_att_check_excl` US participants, `r df.excl_VN$ppts_finished - df.excl_VN$ppts_after_att_check_excl` Vietnam participants, and `r df.excl_CN$ppts_finished - df.excl_CN$ppts_after_att_check_excl` China participants who missed 2 or more attention checks.
We followed 4 exclusion criteria that aim to retain only participants who are influenced by one culture: (1) non-native speakers of English and Vietnamese, respectively, (2) fluent in at least one of the other two study languages (Vietnamese for US participants, English for Vietnamese participants and Chinese participants), (3) have lived outside of the test country (US, Vietnam, or China) for more than two years, and (4) have significant international experience (more than 6 international experiences of 2 days or longer.) We did not use a particular criterion for a language if it would exclude 25% or more of any one sample.
In this round of exclusion, we excluded `r df.excl_US$ppts_after_att_check_excl - df.excl_US$ppts_after_demog_excl` US, `r df.excl_VN$ppts_after_att_check_excl - df.excl_VN$ppts_after_demog_excl` Vietnam participants, and `r df.excl_CN$ppts_after_att_check_excl - df.excl_CN$ppts_after_demog_excl` China participants.
After these exclusions, the US sample included `r df.excl_US$ppts_after_demog_excl` participants (`r df.gender_E %>% filter(gender == "Male") %>% pull(n)`M, `r df.gender_E %>% filter(gender == "Female") %>% pull(n)`F, `r df.gender_E %>% filter(gender == "Non-binary") %>% pull(n)` non-binary, `r df.gender_E %>% filter(gender == "Decline to answer") %>% pull(n)` other), with mean age = `r round(df.age_E$mean_age, 2)` (SD = `r round(df.age_E$sd_age, 2)`) and median age = `r df.age_E$median_age`.
The Vietnam sample included `r df.excl_VN$ppts_after_demog_excl` participants (`r df.gender_V %>% filter(gender == "Nam") %>% pull(n)`M, `r df.gender_V %>% filter(gender == "Nữ") %>% pull(n)`F, `r df.gender_V %>% filter(gender == "Từ chối trả lời") %>% pull(n)` other), with mean age = `r round(df.age_V$mean_age, 2)` (SD = `r round(df.age_V$sd_age, 2)`) and median age = `r df.age_V$median_age`.
The China sample included `r df.excl_CN$ppts_after_demog_excl` participants (`r df.gender_M %>% filter(gender == "男性") %>% pull(n)`M, `r df.gender_M %>% filter(gender == "女性") %>% pull(n)`F, `r df.gender_M %>% filter(gender == "拒绝回答") %>% pull(n)` other), with mean age = `r round(df.age_M$mean_age, 2)` (SD = `r round(df.age_M$sd_age, 2)`) and median age = `r df.age_M$median_age`.[^4]

[^4]: A table summarizing number of participants lost at each round of exclusions is included in the Supplementary Information.

We preregistered a more stringent exclusion criterion where participants were excluded if they missed any attention checks.
However, this led to a small sample size, especially for Vietnam context (US = 109, China = 132, Vietnam = 57).
Our reported results with the less stringent exclusion criterion (detailed above) is largely not different from the results with the preregistered criterion.
Any differences are noted in the Analysis section.

## Stimuli Materials

We adapted stimuli from previous studies to create a set of test triads consisting of a cue, with one thematic and one taxonomic match option.
For example, "cow," "grass," and "chicken," where "cow" is the cue, "grass" is the thematic match, and "chicken" the taxonomic match.
We included 105 such triads, a superset including triads pulled from supplemental information and in-text examples across the literature, and others that we adapted or created <!--(see SI for the full list of stimuli and sources)-->.
We selected triads on the basis of cultural familiarity in the US, Vietnam, and China.
The triads were originally in English; they were translated to Vietnamese and Mandarin by a fluent bilingual speaker in each language.
The translations were then checked for accuracy after backtranslation to English by another fluent bilingual in each language who was naive to the original English versions.
All materials are available at [BLINDED FOR REVIEW].

## Procedure

Each participant completed all 105 triads in sets of 21 trials at a time (10 test triads, 10 filler triads, and 1 attention check per page), by selecting the match most related to the cue ("Which thing is most closely related to the bolded item?" and translated equivalents).
The test triads were presented with 105 filler triads mixed in, to obscure the taxonomic-thematic two-answer forced choice structure of the test stimuli and reduce the likelihood that participants would become aware of the design.
The filler triads were groups of three semantically related words, but where the match options were not distinguished by thematic vs. taxonomic similarity, e.g., cue "bird" with match options "lizard" and "toad." Additionally, we included 10 attention check trials, which were formatted like the test and filler triads but included an instruction instead of a cue item, e.g., "Choose wife" with match options "wife" and "husband." In total, each participant completed 210 similarity judgments and 10 attention check questions, with triads presented in randomized orders that varied between subjects.

## Corpus model

<!-- Our general approach is to build a model of similarity that is based on collocation counts in each language.  -->

<!-- ### Raw lexical co-occurrences -->

<!-- To give an intuition for our model, consider the cow-grass-chicken triad: we counted how many times “cow” and “grass” co-occur within a window of text in each corpus, and compare this to how many times “cow” and “chicken” co-occur. Our similarity prediction is then proportional to the relative frequency of these pairs. For example, if the thematic cow-grass match accounts for 30% of collocations for this triad (with cow-chicken making up the other 70%), then our model predicts, correspondingly, that 30% of responses to the triad will be grass, and the other 70% chicken. We then use a mixed-effects regression to evaluate how well each corpus collocate model predicts participants’ similarity judgments, across triads and languages. -->

<!-- #### Collocate retrieval and coding -->

<!-- For our English co-occurrence metric, we used the online interface of the Corpus of Contemporary American English [@COCA] to retrieve collocation counts. We recorded the raw count of times that any cue-match pair (e.g., cow-grass or cow-chicken) co-occur in a window of 19 words, which is the maximum window size in the online interface, and closest to the sentence co-occurrence metric in our VI corpus collocation counts.  -->

<!-- To determine Vietnamese co-occurrence, we used the raw frequency of sentence co-occurrences from a subset of the Vietnamese corpus in the Leipzig Corpora Collection [@VICorpus]; this corpus includes 70 million Vietnamese sentences, but our corpus data comes from a 1 million sentence subset for which co-occurrence counts are available for download. Vietnamese makes very frequent use of compositional morphology but the written language  uses spacing to delineate syllable boundaries rather than word boundaries. Accordingly, collocate searches returned instances of both target terms and many morphologically related, but distinct, words. We included in our counts any instances of the target term or close semantic neighbors containing the same morpheme(s) as long as they entailed a likely literal reference to the target term. For example, our search for collocates of the term “gà” (chicken) also returned “gà mái,” a distinct word for female chickens. Despite being a different word, “gà mái” both includes the morpheme for “chicken” and entails reference to a chicken. Accordingly, instances of both “gà” and “gà mái” were included in our collocate count for “chicken.” Some compounds do not meet this criteria, and were excluded from collocate counts. For example “trái cây” (tree.fruits) includes the same syllable as the word “cây” (tree), but refers to fruit that comes from trees, not the trees themselves, and was therefore excluded from our collocate counts for “tree.” -->

<!-- Mandarin co-occurrence was collected using the online interface of the Chinese Lexical Association Database (Lin et al, 2019). Mandarin also makes frequent use of compositional morphology. To match the Vietnamese corpus statistics, we included in our counts any instances of the target term or close semantic neighbors containing the same morpheme(s) following the rule described above. These semantic neighbors were either suggested to us by the online interface (which gave search suggestions for words beginning with the target term as we typed them in) or suggested by a native speaker of Mandarin conducting the co-occurrence count data collection. For example, in getting the collocates of the term "門" (door), we also included "門兒" (also door, but is returned as a different word). Compounds that do not meet this criteria were excluded from collocate counts. For example "門牙" (door.tooth) refers to the front tooth even though it includes the same character as the target word "門" (door), so was therefore excluded from our collocate counts for "door". -->

<!-- #### Collocate similarity model -->

<!-- From the raw co-occurrence counts of each triad, we calculated the thematic co-occurrence ratio as the number of thematic co-occurrences over the sum of thematic and taxonomic co-occurrences. We did this for both the English and Vietnamese corpus co-occurrence counts. In this way, we obtained predictions for 73 of 105 triads from the Vietnamese corpus and 104 of 105 triads from English. We therefore limited all analyses reported here to the subset of triads for which our corpus-based model can make meaningful predictions, meaning triads that have at least one non-zero collocate count (either thematic or taxonomic match). We replaced any remaining zero collocate counts with $\epsilon$ to account for sparsity in the corpus data. We then tested whether these simple relative frequency models predict US and VN responding. -->

<!-- ### Cosine distance of word vectors -->

Our general approach is to predict behavioral preferences in similarity judgment using relative similarity between word embeddings.

#### Word vector retrieval

We use the fastText pre-trained models of English, Mandarin, and Vietnamese in @Grave2018.
These models are trained on Common Crawl and Wikipedia using We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText.
These models were trained using a Continuous Bag of Words (CBOW) with position-weights and a window of size 5.
The models use character n-grams of length 5 and 10 negative examples.
From the aforementioned models, we retrieve the word vectors (dimension 300) for each word we are interested in.

#### Similarity model

To give an intuition for our model, consider again the cow-grass-chicken triad: we retrieved word vectors for "cow" and "grass", and calculate the cosine distance between these vectors.
Similarly, we retrieved vectors for "cow" and "chicken" and calculate the cosine distance between them.
Our similarity prediction is then inversely proportional to the ratio of cosine distance of these pairs.
This is because a larger cosine distance means the word vectors are further apart, and thus the words are less similar.
For example, if the cosine distance of thematic cow-grass is 0.7 and the cosine distance of taxonomic cow-chicken is 0.3, then our model predicts, correspondingly, that 30% of responses to the triad will be grass, and the other 70% chicken.

In practice, we calculated the cosine distance between each cue-thematic match (thematic cosine distance) and cue-taxonomic match (taxonomic cosine distance), using the spatial.distance.cosine function from the SciPy package [@2020SciPy].
We then calculated the thematic cosine distance proportion as thematic cosine distance over the sum of taxonomic cosine distance and thematic cosine distance.
We did this for all three corpora.
We were able to obtain predictions for all triads in all languages.
We then use a mixed-effects regression to evaluate how well each corpus model predicts participants' similarity judgments, across triads and cultural contexts.
All analyses are available at [BLINDED FOR REVIEW].

# Results & Discussion

## 1. Replication of previous work and extension to a Vietnamese sample

```{r country, warning=FALSE, cache = TRUE, include=FALSE, paged.print=TRUE}
df.country <- df %>%
  group_by(subject, country) %>%
  summarize(theme_resp_percent = mean(responses_theme, na.rm = T))

df.country_sum <- df.country %>%
  group_by(country) %>%
  summarize(mean_theme_resp_percent = mean(theme_resp_percent), 
            sd_theme_resp_percent = sd(theme_resp_percent))

fit.country = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df, 
                    family = "binomial")
summary(fit.country) 
fit.country.anova = Anova(fit.country, type=3)

fit.country_EN_VN = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df %>% filter(country != "China"),
                    family = "binomial")
summary(fit.country_EN_VN) 
```

Following previous work, we would expect participants from mainland China to prefer thematic matches more than US participants (to use the cow-grass-chicken triad: we would expect participants from mainland China to prefer the cow-grass match over the cow-chicken match to a larger extent compared to US participants).
We would also expect that participants from Vietnam would pattern with China, and also show a significantly higher preference for thematic matches compare to US participants.

The group means of proportion of thematic response in mainland China is the highest (M = `r round((df.country_sum %>% filter(country == "China"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "China"))$sd_theme_resp_percent, 2)`), followed by the groups means in Vietnam (M = `r round((df.country_sum %>% filter(country == "Vietnam"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "Vietnam"))$sd_theme_resp_percent, 2)`), which is slightly higher than that of the US (M = `r round((df.country_sum %>% filter(country == "US"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "US"))$sd_theme_resp_percent, 2)`) (Figure 1).

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.env="figure", fig.align = "center", fig.cap="Proportion of thematic responses by country."}

#show violin plot
ggplot(df.country,
       mapping = aes(x = factor(country, levels=c("China", "Vietnam", "US")), 
                     y = theme_resp_percent, 
                     color = factor(country, levels=c("China", "Vietnam", "US")))) +
  geom_violin() +
  geom_jitter(height = 0, 
              alpha = 0.3) +  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  labs(y = "Proportion Thematic Chosen", 
       x = "Country") + 
  scale_color_manual(values=c("#D63230", "#F39237", "#1C77C3")) + 
  theme(legend.position = "none")
```

To test for cross-context differences in similarity judgments between the countries, we ran a mixed-effects logistic regression predicting triad responding (taxonomic or thematic) with country (US, China, or Vietnam) as a fixed effect.
As random effects, we included an intercept per subject and one per triad, as well as by-triad random slopes for country to account for variation in the country effect across triads.

Overall, there is a significant effect of country on proportion of thematic responses ($\chi^2$(`r fit.country.anova["country", "Df"]`) = `r round(fit.country.anova["country", "Chisq"], 2)`, p `r ifelse( fit.country.anova["country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country.anova["country", "Pr(>Chisq)"], 3)), "< .001")`). However, this effect is driven by the difference between US and China responding ($\beta$ = `r round(fixef(fit.country)["countryUS"], 2)`, p `r ifelse(as.data.frame(summary(fit.country)[["coefficients"]])["countryUS", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country)[["coefficients"]])["countryUS", "Pr(>|z|)"], 3)), "< .001")`). There is no statistical difference between the Vietnam and China responding ($\beta$ = `r round(fixef(fit.country)["countryVietnam"], 2)`, p `r ifelse(as.data.frame(summary(fit.country)[["coefficients"]])["countryVietnam", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country)[["coefficients"]])["countryVietnam", "Pr(>|z|)"], 3)), "< .001")`), and the US and Vietnam responding ($\beta$ = `r round(fixef(fit.country_EN_VN)["countryVietnam"], 2)`, p `r ifelse(as.data.frame(summary(fit.country_EN_VN)[["coefficients"]])["countryVietnam", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country_EN_VN)[["coefficients"]])["countryVietnam", "Pr(>|z|)"], 3)), "< .001")`).

On this analysis, we do not find support that the US-China tendencies toward taxonomic and thematic responding (respectively) extend to the US-Vietnam comparison.
Accordingly, we cannot speak to overall biases toward thematic responding across Asian cultural contexts broadly, but we do replicate the differences documented by @Ji2004 between the US and China.
However, in our corpus model comparison, we do find evidence for different, more fine-grained variation in similarity judgments between the US and Vietnam.

## 2. Language statistics as a predictor for cross-cultural variation in similarity judgments

### Single corpus model

To test whether variation in language statistics can explain differences in similarity judgments between US and Vietnam participants, we compare logistic mixed-effects regression models fit to the thematic responding data from each country separately.
We first ask how well each corpus model (English, Vietnamese, or Mandarin) predicts similarity judgments by speakers of the corresponding language (US, Vietnam, or China).
To do this, we use a mixed-effects logistic regression to predict triad responses (0=taxonomic or 1=thematic) with corpus prediction (proportion of cosine distance) as a fixed effect and participant and triad as random effects.
If environmental statistics (as proxied by language statistics) contribute to the differences in similarity judgments, we would expect each language corpus to be a good predictor for similarity judgment responding in its corresponding context.

```{r US responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
#English
fit.EN_US_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.EN_US_cos)
fit.EN_US_cos.anova = Anova(fit.EN_US_cos, type = 3)
fit.EN_US_cos.anova

#Vietnamese
fit.VI_US_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.VI_US_cos)
fit.VI_US_cos.anova = Anova(fit.VI_US_cos, type = 3)
fit.VI_US_cos.anova

#Mandarin
fit.ZH_US_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.ZH_US_cos)
fit.ZH_US_cos.anova = Anova(fit.ZH_US_cos, type = 3)
fit.ZH_US_cos.anova
```

```{r VN responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
#English
fit.EN_VN_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.EN_VN_cos)
fit.EN_VN_cos.anova = Anova(fit.EN_VN_cos, type = 3)
fit.EN_VN_cos.anova

#Vietnamese
fit.VI_VN_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.VI_VN_cos)
fit.VI_VN_cos.anova = Anova(fit.VI_VN_cos, type = 3)
fit.VI_VN_cos.anova

#Mandarin
fit.ZH_VN_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.ZH_VN_cos)
fit.ZH_VN_cos.anova = Anova(fit.ZH_VN_cos, type = 3)
fit.ZH_VN_cos.anova
```

```{r CN responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
## English
fit.EN_CN_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.EN_CN_cos)
fit.EN_CN_cos.anova = Anova(fit.EN_CN_cos, type = 3)
fit.EN_CN_cos.anova

#Vietnamese
fit.VI_CN_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.VI_CN_cos)
fit.VI_CN_cos.anova = Anova(fit.VI_CN_cos, type = 3)
fit.VI_CN_cos.anova

#Mandarin
fit.ZH_CN_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.ZH_CN_cos)
fit.ZH_CN_cos.anova = Anova(fit.ZH_CN_cos, type = 3)
fit.ZH_CN_cos.anova
```

```{r calculate range of beta statistics for cosine, warning=F, include=F}
beta.single_corpus_cos <- c(
  round(fixef(fit.EN_US_cos)["theme_cosine_prop_E"], 2),
  round(fixef(fit.VI_US_cos)["theme_cosine_prop_V"], 2), 
  round(fixef(fit.ZH_US_cos)["theme_cosine_prop_M"], 2), 
  
  round(fixef(fit.EN_CN_cos)["theme_cosine_prop_E"], 2),
  round(fixef(fit.VI_CN_cos)["theme_cosine_prop_V"], 2), 
  round(fixef(fit.ZH_CN_cos)["theme_cosine_prop_M"], 2), 
  
  round(fixef(fit.EN_VN_cos)["theme_cosine_prop_E"], 2),
  round(fixef(fit.VI_VN_cos)["theme_cosine_prop_V"], 2), 
  round(fixef(fit.ZH_VN_cos)["theme_cosine_prop_M"], 2)
)
```

We found that all corpora are significant predictors of all cultural context responding, with p \< 0.05 and $\beta$ from `r min(beta.single_corpus_cos)` to `r max(beta.single_corpus_cos)`.
(For a full report, see Supplementary Information.)

While each corpus is a good predictor for its corresponding context, the fact that all corpora covary with all cultural contexts suggests that language is such a rich proxy of human experience that even using the wrong proxy (e.g. Mandarin for US responding) is informative.
A single corpus model might therefore not be informative in culture-specific ways, but might only reflect consistency in experiences across cultures.

### Multiple corpora model

If language statistics is able to predict meaningful culture-specific variation in similarity judgment (rather than just consistency across cultures), we would expect each corpus to be the best predictor of its corresponding culture compared to the other two corpora.
We directly compare the corpus models by including both as fixed effects in three mixed-effect regressions (predicting US, Vietnam and China responding) with the same random effects as above.

```{r Eng-Vie-Mand cosine similarities predicting US-VN-CN, warning=FALSE, include=FALSE}

## English/Vietnamese/Mandarin comparison
fit.ENVIZH_US_cos = glmer(responses_theme ~ theme_cosine_prop_E + theme_cosine_prop_V + theme_cosine_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "US"), 
                    family="binomial")
summary(fit.ENVIZH_US_cos)
fit.ENVIZH_US_cos.anova = Anova(fit.ENVIZH_US_cos, type = 3)
fit.ENVIZH_US_cos.anova

fit.ENVIZH_VN_cos = glmer(responses_theme ~ theme_cosine_prop_V + theme_cosine_prop_E + theme_cosine_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "Vietnam"), 
                    family="binomial")
summary(fit.ENVIZH_VN_cos)
fit.ENVIZH_VN_cos.anova = Anova(fit.ENVIZH_VN_cos, type = 3)
fit.ENVIZH_VN_cos.anova

fit.ENVIZH_CN_cos = glmer(responses_theme ~ theme_cosine_prop_M + theme_cosine_prop_E + theme_cosine_prop_V +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "China"), 
                    family="binomial")
summary(fit.ENVIZH_CN_cos)
fit.ENVIZH_CN_cos.anova = Anova(fit.ENVIZH_CN_cos, type = 3)
fit.ENVIZH_CN_cos.anova
```

For US responding: only the English (EN) corpus is a significant predictor[^5].
EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

[^5]: With preregistered exclusion criterion: only the English (EN) and Mandarin (ZH) corpus are significant predictors.

For Vietnam responding: only the Vietnamese (VI) and Mandarin (ZH) corpus are significant predictors[^6].
EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

[^6]: With preregistered exclusion criterion: all three corpora are significant predictors.

For China responding: only the Mandarin (ZH) and English (EN) corpus are significant predictors.
EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

We observed some level of language specificity from this analysis.
The English corpus is the best predictor for US responding, and the Mandarin corpus is the best predictor for China response.
While this is not the case with the Vietnamese corpus and the Vietnam responding, the Vietnamese corpus is still a significant predictor for the Vietnam responding (Figure 2).
These results strengthens our hypothesis that variation in inputs to similarity judgment (as proxied by language statistics) can predict cross-cultural variations of similarity judgment.

```{r echo=FALSE, warning = FALSE}
coeff_value_cos <- c(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_V"], 
                 fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_M"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_V"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_M"], 
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_V"], 
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_M"])

se_value_cos <- c(summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_M", "Std. Error"], 
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_M", "Std. Error"],
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_M", "Std. Error"])

country_value <- c(rep("US", 3),
                   rep("VN", 3),
                   rep("CN", 3))

corpus_value <- c(rep(c("EN", "VI", "ZH"), 3))

df.USVNCN_coeffs_cos <- data.frame(country_value, corpus_value, coeff_value_cos, se_value_cos) %>%
  mutate(corpus_value = factor(corpus_value, levels=c("ZH", "EN", "VI")))

plot.coeffs_cos <- ggplot(data = df.USVNCN_coeffs_cos, 
       mapping = aes(x = country_value, y = coeff_value_cos, fill = corpus_value)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin= coeff_value_cos - se_value_cos, ymax = coeff_value_cos + se_value_cos), width=.2,
                 position=position_dodge(.9)) +
  scale_y_reverse() + 
  labs(x = "Country", y = "Fixed effect size", fill = "Corpus") + 
  scale_x_discrete(labels=c("CN" = "China", "US" = "US", "VN" = "Vietnam")) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237"))
```

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.env = "figure", fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap="Fixed effect sizes of each corpus lexical statistics (cosine distance proportion) when included as a predictor for China, US, and Vietnam responding, respectively. The English corpus is the best predictor for US response, and the Mandarin corpus is the best predictor for China response."}
#plot.coeffs_freq + 
  plot.coeffs_cos
```

```{r model comparison, include=FALSE}
fit.US_cos_compare = anova(fit.ENVIZH_US_cos, fit.EN_US_cos, type = 3)
fit.VN_cos_compare = anova(fit.ENVIZH_VN_cos, fit.VI_VN_cos, type = 3)
fit.CN_cos_compare = anova(fit.ENVIZH_CN_cos, fit.ZH_CN_cos, type = 3)
```

However, in all cultural contexts, adding the other two corpora produces a significantly better fit than the identical model without the additional corpora, and only the corresponding corpus included as a predictor (US response: $\chi^2$(`r fit.US_cos_compare["fit.ENVIZH_US_cos", "Df"]`) = `r round(fit.US_cos_compare["fit.ENVIZH_US_cos", "Chisq"], 2)`, p `r ifelse( fit.US_cos_compare["fit.ENVIZH_US_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.US_cos_compare["fit.ENVIZH_US_cos", "Pr(>Chisq)"], 3)), "< .001")`; Vietnam response: $\chi^2$(`r fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Df"]`) = `r round(fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Chisq"], 2)`, p `r ifelse( fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"], 3)), "< .001")`; China response: $\chi^2$(`r fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Df"]`) = `r round(fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Chisq"], 2)`, p `r ifelse( fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Pr(>Chisq)"], 3)), "< .001")`). This analysis suggests that culture-specific inputs to similarity judgment (as proxied by language statistics) do not fully explain cross-cultural differences in similarity judgment.

<!-- ## 3. Cultural context as a predictor for cross-cultural variation in similarity judgments -->

<!-- Our above models tested whether language statistics is a good predictor for similarity judgment. -->
<!-- As we noted above, language statistics can be a proxy of both inputs and conceptualization of similarity. -->
<!-- However, it is still an open question whether conceptualization of similarity itself has a unique contribution to similarity judgment (beyond its influence on language statistics). -->
<!-- To test this hypothesis, we operationalize cultural-specific conceptualization of similarity as country. -->
<!-- We then compare a model that includes country, corresponding corpus statistics and their interaction as fixed effects to a model with only the corresponding corpus statistics as the fixed effect. -->
<!-- If conceptualization of similarity has a unique contribution to similarity judgment, we should see that terms including country to be a significant predictor of responding. -->

```{r add corresponding language, include = F}
df <- df %>%
  mutate(theme_freq_prop_corr_lang = case_when(
    country == "US" ~ theme_freq_prop_E,
    country == "China" ~ theme_freq_prop_M,
    country == "Vietnam" ~ theme_freq_prop_V)) %>%
    mutate(theme_cosine_prop_corr_lang = case_when(
    country == "US" ~ theme_cosine_prop_E,
    country == "China" ~ theme_cosine_prop_M,
    country == "Vietnam" ~ theme_cosine_prop_V))
```

<!-- ```{r country language interaction in raw co-occurrences, include = F} -->

<!-- fit.language = glmer(responses_theme ~ theme_freq_prop_corr_lang + (1 | subject) + (1 | triad),  -->

<!--           data = df %>% filter(!(triad %in% triads_omit)),   -->

<!--           family="binomial") -->

<!-- summary(fit.language) -->

<!-- fit.language.anova = Anova(fit.language, type = 3) -->

<!-- fit.language.anova -->

<!-- fit.country_language = glmer(responses_theme ~ theme_freq_prop_corr_lang * country + (1 | subject) + (1 | triad),  -->

<!--           data = df %>% filter(!(triad %in% triads_omit)),   -->

<!--           family="binomial") -->

<!-- summary(fit.country_language) -->

<!-- fit.country_language.anova = Anova(fit.country_language, type = 3) -->

<!-- fit.country_language.anova -->

<!-- fit.country_language_compare = anova(fit.language, fit.country_language, type = 3) -->

<!-- fit.country_language_compare -->

<!-- ``` -->

```{r country language interaction in cosine prop, include = F}
fit.language_cos = glmer(responses_theme ~ theme_cosine_prop_corr_lang + (1 | subject) + (1 | triad),
          data = df,
          family="binomial")
summary(fit.language_cos)
fit.language_cos %>% joint_tests()
fit.language_cos.anova = Anova(fit.language_cos, type = 3)
fit.language_cos.anova
#report(fit.language_cos)

fit.country_language_cos= glmer(responses_theme ~ theme_cosine_prop_corr_lang * country + (1 | subject) + (1 | triad),
          data = df,
          family="binomial")
summary(fit.country_language_cos)
fit_joint.country_language_cos <- fit.country_language_cos %>%
  joint_tests() %>%
  remove_rownames() %>%
  column_to_rownames(var = "model term")
fit.country_language_cos.anova = Anova(fit.country_language_cos, type = 3)
fit.country_language_cos.anova

fit.country_language_cos_compare = anova(fit.language_cos, fit.country_language_cos, type = 3)
fit.country_language_cos_compare
```

<!-- In the model containing only the corresponding corpus statistics, the corpus statistics is a significant predictor ($\beta$ = `r round(fixef(fit.language_cos)["theme_cosine_prop_corr_lang"], 2)`, $\chi^2$(`r fit.language_cos.anova["theme_cosine_prop_corr_lang", "Df"]`) = `r round(fit.language_cos.anova["theme_cosine_prop_corr_lang", "Chisq"], 2)`, p `r ifelse( fit.language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"], 3)), "< .001")`). When adding country and interaction between country and corpus, corpus statistics ($\beta$ = `r round(fixef(fit.country_language_cos)["theme_cosine_prop_corr_lang"], 2)`, $\chi^2$(`r fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Df"]`) = `r round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Chisq"], 2)`, p `r ifelse( fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"], 3)), "< .001")`), country (F(`r fit_joint.country_language_cos["country", "df1"]`, `r fit_joint.country_language_cos["country", "df2"]`) = `r round(fit_joint.country_language_cos["country", "F.ratio"], 2)`, $\chi^2$(`r fit.country_language_cos.anova["country", "Df"]`) = `r round(fit.country_language_cos.anova["country", "Chisq"], 2)`, p `r ifelse( fit.country_language_cos.anova["country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_language_cos.anova["country", "Pr(>Chisq)"], 3)), "< .001")`), and interaction between country and corpus (F(`r fit_joint.country_language_cos["theme_cosine_prop_corr_lang:country", "df1"]`, `r fit_joint.country_language_cos["theme_cosine_prop_corr_lang:country", "df2"]`) = `r round(fit_joint.country_language_cos["theme_cosine_prop_corr_lang:country", "F.ratio"], 2)`, $\chi^2$(`r fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Df"]`) = `r round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Chisq"], 2)`, p `r ifelse( fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_language_cos.anova["theme_cosine_prop_corr_lang:country", "Pr(>Chisq)"], 3)), "< .001")`), are significant predictors[^7]. Including the country and country-corpus terms to the language-only model significantly its ability to explain variation in responding ($\chi^2$(`r fit.country_language_cos_compare["fit.country_language_cos", "Df"]`) = `r round(fit.country_language_cos_compare["fit.country_language_cos", "Chisq"], 2)`, p `r ifelse( fit.country_language_cos_compare["fit.country_language_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_language_cos_compare["fit.country_language_cos", "Pr(>Chisq)"], 3)), "< .001")`). This result points to a unique contribution by culture-specific ways of conceptualizing similarity to similarity judgment that is not captured in language statistics. -->

<!-- [^7]: With preregistered exclusion criterion: only corpus statistics and interaction between country and corpus are significant predictors. -->

## 3. Language statistics as a predictor for non-structured filler items

A possible concern with our approach is that our models using language statistics in Q2 are only able to account for variation in thematic/taxonomic preference in similarity judgment because language statistics is picking up on differences in thematic/taxonomic preference in conceptualization of similarity. For example, perhaps "cow" and "grass" co-occurs more than "cow" and "chicken" in Mandarin because Chinese speakers prefer thematic relations, and thus talk about "cow" and "grass" together more often. Vice versa, English speakers preferring taxonomic relations would talk about "cow" and "chicken" together more often, hence driving higher lexical co-occurrence for "cow" and "chicken" in the English corpus. In other words, perhaps the language statistics models are merely measuring variation in thematic/taxonomic relation preference in conceptualization of similarity, only in a different way. If this is the case, language statistics should not be able to predict responding for filler items (such as whether "tomorrow" or "yesterday" is more related to "today") because these items do not have a thematic/taxonomic structure. Additionally, even when one option might be more related to the cue, such relationship is not systematic throughout the set of fillers. On the other hand, if language statistics capture similarity judgment in a more general sense (beyond thematic/taxonomic preference in conceptualization of similarity), it should also be a significant predictor for the non-structured filler items. 

```{r incorporate base freq for fillers, warning = F, include = F}
df.filler <- read.csv("../../../data/data_filler_USCNVN_ENZHVI.csv")
fillers_omit <- read.csv("../../../data/fillers_omit.csv") %>%
  pull(x)

df.filler <- df.filler %>%
  mutate(word1_match_freq_prop_corr_lang = ifelse(country == "US", 
                                                  word1_match_frequency_prop_E, 
                                                  ifelse(country == "China", 
                                                         word1_match_frequency_prop_M, 
                                                         word1_match_frequency_prop_V))) %>% 
  mutate(word1_match_cosine_prop_corr_lang = ifelse(country == "US", 
                                                  word1_match_cosine_prop_E, 
                                                  ifelse(country == "China", 
                                                         word1_match_cosine_prop_M, 
                                                         word1_match_cosine_prop_V)))
  
           
           
#NOT SURE WHY THIS DOESN'T WORK
    #        case_when(
    # country == "US" ~ word1_match_frequency_prop_E,
    # country == "China" ~ word1_match_frequency_prop_M, 
    # country == "Vietnam" ~ word1_match_frequency_prop_V)) %>% 
    # mutate(word1_match_cosine_prop_corr_lang = case_when(
    # country == "US" ~ word1_match_cosine_prop_E,
    # country == "China" ~ word1_match_cosine_prop_M, 
    # country == "Vietnam" ~ word1_match_cosine_prop_V))
```

```{r country summary filler, warning=FALSE, cache = TRUE, include = FALSE, paged.print=TRUE}
df.country_filler <- df.filler %>%
  group_by(subject, country) %>%
  summarize(word1_resp_percent = mean(responses_word1, na.rm = T))

df.country_sum_filler <- df.country_filler %>%
  group_by(country) %>%
  summarize(mean_word1_resp_percent = mean(word1_resp_percent), 
            sd_word1_resp_percent = sd(word1_resp_percent))
```

```{r echo=FALSE, warning=FALSE, include = F, cache = TRUE, fig.env="figure", fig.align = "center", fig.cap="Proportion of Word1 responses by country."}
#show violin plot
ggplot(df.country_filler,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = word1_resp_percent, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_violin() +
  geom_jitter(height = 0, 
              alpha = 0.3) +  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  labs(y = "Proportion Word1 Chosen", 
       x = "Country") + 
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237")) + 
  theme(legend.position = "none")
```

```{r warning=FALSE, include = F, cache = TRUE, fig.env="figure", paged.print=TRUE}
# df.country_sum <- df.country %>%
#   group_by(country) %>%
#   summarize(mean_theme_resp_percent = mean(theme_resp_percent), 
#             sd_theme_resp_percent = sd(theme_resp_percent))

fit.country_filler = glmer(responses_word1 ~ country + (1 | subject) + (country | cue),
                    data = df.filler, 
                    family = "binomial")
summary(fit.country_filler) 
fit.country_filler.anova = Anova(fit.country_filler, type=3)
fit.country_filler.anova
```

For each filler triad, we randomly assigned one of the responding options as 'Word1'.
Running a mixed-effects logistic regression to predict responding (Word1 or Word2) with country as the fixed effect and a random effect structure equivalent to Q1, we found no effect of country on filler responding ($\chi^2$(`r fit.country_filler.anova["country", "Df"]`) = `r round(fit.country_filler.anova["country", "Chisq"], 2)`, p `r ifelse( fit.country_filler.anova["country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country_filler.anova["country", "Pr(>Chisq)"], 3)), "< .001")`).

<!-- #### Raw co-occurrences -->

<!-- ```{r US responding predicted by each corpus model raw co-occurrences for fillers, warning=FALSE,include = F} -->

<!-- # English  -->

<!-- fit.EN_US_filler = glmer(responses_word1 ~ word1_match_frequency_prop_E + (1 | subject) + (1 | cue),  -->

<!--           data = df.filler %>% filter(country == "US",  -->

<!--                                !(cue_id %in% fillers_omit)),  -->

<!--           family="binomial") -->

<!-- summary(fit.EN_US_filler) -->

<!-- fit.EN_US_filler.anova = Anova(fit.EN_US_filler, type = 3) -->

<!-- fit.EN_US_filler.anova -->

<!-- ``` -->

<!-- ```{r VN responding predicted by each corpus model raw co-occurrences for fillers, warning=FALSE, include = F} -->

<!-- # Vietnamese -->

<!-- fit.VI_VN_filler = glmer(responses_word1 ~ word1_match_frequency_prop_V + (1 | subject) + (1 | cue),  -->

<!--           data = df.filler %>% filter(country == "Vietnam",  -->

<!--                                !(cue_id %in% fillers_omit)),  -->

<!--           family="binomial") -->

<!-- summary(fit.VI_VN_filler) -->

<!-- fit.VI_VN_filler.anova = Anova(fit.VI_VN_filler, type = 3) -->

<!-- fit.VI_VN_filler.anova -->

<!-- ``` -->

<!-- Yes, corpus raw co-occurrences predicts responding for both US and Vietnamese. *need to discuss with Shan to incorporate Mandarin due to a lot of empty counts* -->

<!-- #### Cosine distance -->

```{r US responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# English 
fit.EN_US_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_E + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "US"), 
          family="binomial")
summary(fit.EN_US_filler_cos)
fit.EN_US_filler_cos.anova = Anova(fit.EN_US_filler_cos, type = 3)

fit.EN_US_filler_cos.anova
```

```{r VN responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# Vietnamese
fit.VI_VN_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_V + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.VI_VN_filler_cos)
fit.VI_VN_filler_cos.anova = Anova(fit.VI_VN_filler_cos, type = 3)

fit.VI_VN_filler_cos.anova
```

```{r CN responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# Mandarin
fit.ZH_CN_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_M + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "China"), 
          family="binomial")
summary(fit.ZH_CN_filler_cos)
fit.ZH_CN_filler_cos.anova = Anova(fit.ZH_CN_filler_cos, type = 3)

fit.ZH_CN_filler_cos.anova
```

```{r calculate range of beta statistics for cosine fillers, warning=F, include=F}
beta.filler_single_corpus_cos <- c(
  round(fixef(fit.EN_US_filler_cos)["word1_match_cosine_prop_E"], 2),
  round(fixef(fit.ZH_CN_filler_cos)["word1_match_cosine_prop_M"], 2), 
  round(fixef(fit.VI_VN_filler_cos)["word1_match_cosine_prop_V"], 2)
)
```

Using the same mixed-effects logistic regression structure as the single corpus models in Q2, we predict responses (1=word1 or 0=word2) of each cultural context with the corresponding corpus statistics as a fixed effect, and participant and triad as random effects.
We found that in all cultural contexts, the corresponding corpus is a significant predictor of responding, with p \< 0.05 and $\beta$ from `r min(beta.filler_single_corpus_cos)` to `r max(beta.filler_single_corpus_cos)`.
(For a full report, see Supplementary Information.) These results show that the language statistics model is accounting for general similarity in addition to structured thematic/taxonomic similarity. 

```{r filler corresponding cosine, warning = F, include = F}
fit.filler = glmer(responses_word1 ~ word1_match_cosine_prop_corr_lang + (1 | subject) + (1 | cue_id), 
          data = df.filler,
          family="binomial")
summary(fit.filler)
fit.filler.anova = Anova(fit.filler, type = 3)

fit.filler.anova
#report(fit.filler)
```

```{r combined filler and stimulus, warning = F, include = F}
df.triad_filler <- rbind(
  df %>% mutate(response = responses_theme, 
                cosine_prop_corr_lang = theme_cosine_prop_corr_lang, 
                cue_id = triad, 
                is_filler = 0) %>%
          select(response, cosine_prop_corr_lang, is_filler,
                 cue_id, country, subject), 
  df.filler %>% mutate(response = responses_word1,
                       cosine_prop_corr_lang = word1_match_cosine_prop_corr_lang,
                       is_filler = 1) %>%
                select(response, cosine_prop_corr_lang, is_filler,
                       cue_id, country, subject)
)

fit.triad_filler = glmer(response ~ cosine_prop_corr_lang * is_filler + (1 | subject) + (1 | cue_id), 
          data = df.triad_filler,
          family="binomial")
summary(fit.triad_filler)
fit.triad_filler.anova = Anova(fit.triad_filler, type = 3)

fit.triad_filler.anova
```

To investigate whether language statistics predicts triads and filler items differently, we compare the variance in responding that can be accounted for by language statistics of the corresponding corpus in the two different types of items (structured triads versus non-structured filler items). Namely, we ran two mixed-effects logistic regression models that predict responding across cultural contexts (1=thematic or word1, 0=taxonomic or word2), with the corresponding corpus statistics (for e.g., cosine proportion from the English corpus for US responding) as a fixed effect and participant and item as random effects.
Consistent with our results above, corresponding corpus is a significant predictor for responding to both triads and filler items (triads: $\beta$ = `r round(fixef(fit.language_cos)["theme_cosine_prop_corr_lang"], 2)`, $\chi^2$(`r fit.language_cos.anova["theme_cosine_prop_corr_lang", "Df"]`) = `r round(fit.language_cos.anova["theme_cosine_prop_corr_lang", "Chisq"], 2)`, p `r ifelse( fit.language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.language_cos.anova["theme_cosine_prop_corr_lang", "Pr(>Chisq)"], 3)), "< .001")`, fillers: $\beta$ = `r round(fixef(fit.filler)["word1_match_cosine_prop_corr_lang"], 2)`, $\chi^2$(`r fit.filler.anova["word1_match_cosine_prop_corr_lang", "Df"]`) = `r round(fit.filler.anova["word1_match_cosine_prop_corr_lang", "Chisq"], 2)`, p `r ifelse(fit.filler.anova["word1_match_cosine_prop_corr_lang", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.filler.anova["word1_match_cosine_prop_corr_lang", "Pr(>Chisq)"], 3)), "< .001")`). Importantly, we found comparable conditional $R^2$ when corresponding corpus is used to predict only triad items ($R^2$ = `r round(as.data.frame(r.squaredGLMM(fit.language_cos))["theoretical", "R2c"], 2)`) and only filler items ($R^2$ = `r round(as.data.frame(r.squaredGLMM(fit.filler))["theoretical", "R2c"], 2)`), suggesting that culture-corresponding corpus statistics is able to explain the same amount of variance in the responding data for either triads or filler items. This result provides evidence for our view that language statistics is measuring general tendencies in similarity judgment, beyond preference for thematic/taxonomic relations.

# General Discussion

In this paper, we consider whether statistics of the environment (as proxied by language statistics) can account for cross-cultural differences in a classic similarity judgment paradigm, as an alternative to the view that members of different cultures vary in their conceptualization of similarity.
We replicated the previously documented contrast between English speakers in the US and Mandarin Chinese speakers from East Asia (mainland China, Taiwan, Hong Kong, and Singapore), with mainland China participants preferring thematic relations to a greater extent compared to US participants. Our sample of Vietnamese participants showed intermediate response but not significantly different from either Chinese or US participants.
This finding suggests some limitations on the generality of the cultural account, which proposes that thematic/taxonomic similarity preference aligns with East Asian/Western tendency for holistic vs analytic processing.
We found some support for the environmental statistics account: each corpus statistics is a good predictor for the corresponding country's similarity judgments, even when other corpus statistics are included, and even with triads without a thematic/taxonomic structure.
<!-- However, we also found that culture-based conceptualization of similarity uniquely contributes to predicting similarity judgment when included in a model with corpus statistics. -->
Overall, our results provide evidence that cross-cultural differences in similarity judgment are related to linguistic co-occurrence patterns (which also vary across cultures).

There are some important limitations of our approach.
While we discuss cross-cultural variability at the level of countries or larger world areas, these are not cultural monoliths.
For convenience, we operationalize culture at the level of country, based on where participants were raised.
It is an open question whether performance in our participant populations (of relatively young and well-educated adults) is representative of the broader country.
This is especially true for societies with substantial ethnic and cultural variation such as the US.
We expect that our data is likely to underestimate variation both within and between the countries we sample from.

Additionally, language, culture, cognition, and individual experiences are intertwined in complex causal relationships.
In this study, we measure language and its relation to cross-cultural differences in categorization, but these relations test only the plausibility of a language-based account; they cannot establish the direction of causality.

@Ji2004 established that culture-aligned differences in this paradigm exist, even when the test language is held constant, concluding that "it is culture (independent of the testing language) that led to different grouping styles" in their study. Our data provide a cautionary note to this conclusion, suggesting that while cultural differences in similarity judgment exists, we might get better traction at modelling cross-cultural variation through environmental statistics such as lexical co-occurrences, rather than treating groups of cultures as monoliths (East Asian vs Western). However, there are still many open questions for this account. Our operationalization of environmental statistics with lexical co-occurrence is a proxy of both language inputs and culture-specific conceptualization of similarity. The extent to which cross-cultural differences in similarity judgment are driven by language versus culture is still an open question. Future work should also aim to provide a more specific computational account of how lexical co-occurrence might guide categorization preference beyond the simple proportion-of-similarity model tested here, and investigate other individual- or triad-specific factors that might influence how environmental statistics affect similarity judgment.

Despite these caveats, our findings here demonstrate the plausibility of an alternative perspective on cross-cultural accounts of language, thought, and similarity in the case of taxonomic and thematic reasoning: that it may be the input to similarity judgments, rather than the evaluative process or the conceptualization of similarity that produces variation in similarity reasoning across cultural and linguistic contexts.
We hope this work provides a foundation for further research probing this question.

# Acknowledgements
BLINDED FOR PEER REVIEW.

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

```{=tex}
\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
```
\noindent

\newpage
