---
title: "Re-examining cross-cultural similarity judgements using lexical co-occurrence"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

header-includes:
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1, T5]{fontenc}
  
author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
 Is “cow” more closely related to “grass” or “chicken”? Speakers of different languages judge similarity in this context differently, but why? One possibility is that cultures co-varying with these languages induce differences in conceptualizations of similarity. Specifically, East Asian cultures may promote reasoning about thematic similarity, by which cow and grass are more related, whereas Western cultures may bias judgements toward taxonomic relations, like cow-chicken. This difference in notions of similarity is the consensus interpretation for cross-cultural variation in this paradigm. We consider, and provide evidence for, an alternative possibility, by which notions of similarity are similar across contexts, but the statistics of the environment vary. On this account, similarity judgements are guided by co-occurrence in experience, and observing or hearing about cows and grass or cows and chickens more often could induce preferences for these groupings, and account in part for apparent differences in notions of similarity across contexts.
    
keywords: >
    similarity; culture; language; semantics; lexical co-occurrence; variation
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)
```

```{r, libraries, cache=F}
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

```{r setup, include = FALSE, cache=F}
library("papaja")
library("knitr") # for knitting things
library("tidyverse") # for all things tidyverse
library("car")
library("lme4")
library("patchwork")
library("effsize")
library("irr")
require(agreement)

# # these options here change the formatting of how comments are rendered
# opts_chunk$set(
#   comment = "",
#   results = "hold",
#   fig.show = "hold")

# set the default ggplot theme 
theme_set(theme_classic())

#r_refs("r-references.bib")
#r_refs("references/packages.bib")

```

```{r analysis-preferences, cache=F}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction 
By virtue of discrete words and grammatical features, language provides a categorical partition of our continuous experiences. By measuring the similarity between words (as commonly done with lexical co-occurrence models), it may be possible to model the shape of similarity space in order to capture cross-linguistic and cross-cultural variation and provide a more general answer to the question of how language influences our conception of the world.

Taxonomic and thematic similarity provide a convenient entry point to broader debates about cross-linguistic and cross-cultural variation in notions of similarity. Taxonomic categorization is based on the similarity of attributes, for example, similar perceptual properties, like shared color or shape, among objects. In contrast, thematic categorization is based on causal, spatial, and temporal relationships among objects [@Markman1984]. 

## Cross-cultural variation in similarity
Preferences for taxonomic and thematic similarity vary across cultures. @Chiu1972 found that Chinese children (9-10 years old) are more likely to choose thematic matches in a picture triad task (shown, e.g., images of an ant, a bee, and honey) than their American counterparts. These cross-cultural differences are also observed in novel object categorization, with Chinese participants preferring to group by family resemblance across multiple features and Americans preferring a single-feature rule [@Norenzayan2002]. 
<!-- Because differences in similarity preference persist with novel stimuli (as opposed to familiar items), this suggests that participants from different cultural backgrounds use different reasoning strategies when making similarity judgments. -->
The authors link these differences to tendencies toward analytic processing in Western cultures, which emphasizes objects and their properties, and holistic processing in East Asian cultures, which emphasizes relationships between objects and their context (see also @Nisbett2003). In related work, East Asian participants show a higher level of sensitivity to context than their Western counterparts when reproducing drawings from memory [@Ji2000]; visually exploring naturalistic scenes [@Chua2005]; describing scenes [@Masuda2001]; and in explaining the causes of ambiguous behaviors [@Choi1999]. 

@Ji2004 asked whether differences in analytic and holistic processing <!--they use that language, right?--> are driven by language, culture, or a combination of these. They presented Chinese and European American adults with a word triad task, and found a preference for thematic matching among Chinese participants compared to European Americans. They found that test language contributed to this difference, but did not fully explain it: Chinese participants showed a stronger thematic preference when tested in Mandarin, but still showed a thematic preference when tested in English. Ji et al. conclude that culture (independent of language) leads to different styles of reasoning about similarity, whereas language serves as a “tuning” mechanism operating within a culturally-specific style, by activating representations corresponding to the language being used.^[But note that this work did not include a manipulation to prevent participants from engaging in e.g., covert naming in a language other than the test language.] 

This view ascribes cross-cultural differences in similarity judgment to variation in the conceptualization of similarity itself. Alternatively, these judgments could be shaped by cross-cultural differences in the statistics of the environment, and accordingly the content of everyday experiences. Perhaps when confronted with the triad task, participants from all cultures follow the same process for reasoning about similarity, but rely on language or culture-specific inputs to this process. <!--For example, it may be that Vietnamese participants are more likely to have knowledge of farming practices than Americans, and may associate cows and grass more strongly (as grass is the typical feed for cows in the small-scale family farming contexts common in Vietnam). Americans, on the other hand, may have strong associations between cows and chickens as a result of encountering them together in children’s books or petting zoo contexts.--> If we observe a difference in categorization between East Asian and Western participants, it could be that members of both groups use the same procedure (considering similarity that is influenced by both taxonomic and thematic relations), but the inputs to this procedure differ between cultures, with East Asian participants exposed to more instances of thematic similarity than their Western counterparts.


## Estimating variation in experience
While co-occurrences in experience are difficult to measure, co-occurrence in language can provide a rough proxy--and indeed, language may afford many of the “experiences” that people have with infrequently encountered items, like cows or helicopters. In this study, we use co-occurrence in language as a proxy of cultural experience. This is a generalizing assumption, and while no linguistic corpus can be expected to fully capture a culture, using this proxy as a model of culture provides a conservative test of the hypothesis that differences in the input, rather than the process, of reasoning produces cultural variation in this task. This means that if such a model does work -- if linguistic co-occurrence can predict cultural variation in this similarity task without building differences in the decision process into the model -- it is relatively strong evidence that language alone (perhaps as a more easily observed source of information on broader cultural and ecological variation) may account for these cross-cultural differences in reasoning. 

Indeed, previous work suggests that using lexical co-occurrence as a proxy can be useful in thinking about similarity reasoning. Natural language processing tasks have found that lexical co-occurrence is a good predictor for performance in similarity judgment. @Griffiths2007 showed that a model that takes word-document co-occurrence as input can be used to predict word association and the effects of semantic association on a variety of linguistic tasks. Additionally, lexical semantic models that are constructed using lexical co-occurrence (as opposed to annotated relations) have been shown to perform well on predicting human judgments about similarity between word pairs that are thematically or taxonomically related [@Rohde2006].

Our study uses two measures of lexical co-occurrence: 1) raw lexical co-occurrence and 2) cosined distances of fastText word vectors. fastText is a system that uses lexical co-occurrence information to generate a vector representing each word in its lexicon [@Mikolov2018]. 

<!-- MOVE TO METHODS? fastText uses a continuous bag-of-words model, which takes into account a symmetric window of words surrounding the word in question and maximizes the log-likelihood of the probability of the word given its context. Calculating the cosine-similarity between word vectors from fastText gives us a score of how similar the contexts of these words are. Therefore, words that occur in the same context (high lexical co-occurrence) would have a larger cosine-similarity score. 

It must be noted that fastText supplements the lexical co-occurrence information by simultaneously training weight vectors for each position relative to the word in question, as well as adding sub-word information (bag-of-character n-gram vectors for each character forming the words). The similarity judgment prediction given by fastText may be more higher than a model using purely lexical co-occurrence information. However, models that use only lexical co-occurrence information (with or without a simple position weight system that gives more weight to words that are closer to the word in question) also perform well in similarity prediction tasks. For this study, due to the lack of large cleaned corpora for Mandarin and Vietnamese, we leverage fastText pre-trained models on the languages we are interested in @Grave2018.-->

<!-- condense to 2 sentences --> Systems like fastText and word2vec have been demonstrated to be good predictors for similarity judgments. @Liu2019 showed that fastText performed well as the data for a system matching responses in a knowledge base by how similar they are to a customer’s query. A study by @Jatnika2019 tested a word2vec model trained on the English wikipedia corpus against 353 word pairs labeled with similarity values based on human judgment. The word2vec model correlates moderately with human judgment. fastText has also been shown to be sensitive towards cultural effects on word meanings: @Thompson2020 applied fastText to language-specific Wikipedia corpus (among others) to generate a “semantic neighborhood” of 1010 meanings in different languages. The study showed that languages that are spoken by more similar cultures, more geographically proximate and/or historically related had more semantic neighborhoods that aligned. This is a proof of concept for our use of fastText in different languages with varying levels of relatedness.

## The present study
<!-- @Ji2004 observed a difference in similarity judgment between Chinese and European Americans (the former preferred thematic matching the latter taxonomic). However, their evidence is not sufficient to conclude that it is culture that drives this difference, as their cross-cultural data is correlational. Furthermore, they have yet to rule out all mechanisms by which language could be relevant to the task. For example, their language manipulation (bilingual speakers from Mainland China making more taxonomic categorizations when tested in language) only points to an offline effect of language because it does not prevent participants from accessing linguistic representations. A stronger claim could have been made if verbal interference had been conducted, and if cross-cultural differences had remained. This would have been stronger in eliminating online effect of language as a potential cause for cross-cultural differences in similarity judgment.  -->

<!-- Additionally, outside of language, cross-cultural differences could come from a couple different mechanisms:  -->
@Ji2004 suggest that the notion of similarity is what varies across cultural groups, but it is possible that similarity is regarded in the same way and it is actually the input to this similarity judgment that varies across cultural contexts. It may be possible to gain traction on potential mechanisms by examining whether variation in similarity judgments co-vary with environmental statistics that differ across cultural and linguistic contexts.

In this study, we ask whether variation in lexical co-occurrence can account for the cross-context (culture and language) differences observed in how people evaluate similarity. First, we attempt to replicate @Ji2004 by running a study comparing English speakers in the US and Mandarin speakers in mainland China. Second, we evaluate whether the differences observed between English and Mandarin speakers’ similarity judgments also extend to comparisons between English and Vietnamese, by running a similar study between English speakers in the US and Vietnamese speakers in Vietnam. Vietnam is a Southeast Asian country that borders China and is historically greatly influenced by Chinese culture [@Hui2002]. Therefore, it serves as a suitable cultural context to investigate whether the claim made by @Ji2004 and previous studies, that Eastern and Western cultures have different notions of similarity, extends beyond mainland China. 

We then ask to what extent similarity judgments show cross-context differences that align with co-occurrence patterns in participants’ languages. We would only see a main effect of cultural context if differing notions of similarity drive these cross-cultural differences. Meanwhile, if we do see variation in responding trial-to-trial that tracks with lexical co-occurrence statistics (as a rough proxy combining both linguistic and cultural context), this suggests that cross-cultural notions of similarity might be similar, and it is environmental statistics that guide the differences in similarity judgments. 

To preview our results, we find that, while we do not see overall group differences (between the East Asian contexts and the US) in similarity, we do find language-specific co-occurrence can indeed explain cross-context differences in similarity judgments. In contrast to previous accounts, by which culture induces differing conceptions of similarity (varying between taxonomic and thematic), these findings provide an alternative, and more specific, account by which language may explain these cross-cultural differences without invoking variation in notions of similarity. 

There are several important limitations of our approach. While we discuss cross-cultural variability at the level of countries or larger world areas, these are not cultural monoliths. For convenience, we operationalize culture at the level of country, based on where participants were raised. It is an open question whether performance in our participant populations (of relatively young and well-educated adults) is representative of the broader country. This is especially true for societies with substantial ethnic and cultural variation such as the US. We expect that our data is likely to underestimate variation both within and between the countries we sample from.

Language, culture, cognition, and individual experiences are intertwined in complex causal relationships: cultural features (farming) can lead to differences in individuals’ experiences (seeing cows and grass) and in turn these can influence language (talking more about cows and grass). But cultural features (a thematic orientation) could also more directly cause individuals to talk differently about the same experiences (mentioning what cows eat rather than what other animals cows are like). In this study, we measure language and its relation to cross-cultural differences in categorization, but these relations test only the plausibility of a language-based account; they cannot establish the direction of causality.

# Methods 
```{r get responding and demographic data, include = F}
df <- read.csv("../../../data/data_USCNVN_ENZHVI.csv")

df_main_analysis <- df
df.demog <- read.csv("../../../data/responding/data_demog.csv") %>%
  filter(subject %in% unique(df$subject))

triads_omit <- read.csv("../../../data/triads_omit.csv") %>%
  pull(x)
```

```{r get exclusions data, include = F, cache = T}
df.excl_US <- read.csv("../../../data/responding/dataUS_excl_metadata.csv")
df.excl_CN <- read.csv("../../../data/responding/dataCN_excl_metadata.csv")
df.excl_VN <- read.csv("../../../data/responding/dataVN_excl_metadata.csv")
```

```{r demog_E, include = F, cache = T}
df.demog_E <- df.demog %>%
  filter(country == "US")

df.age_E <- df.demog_E %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_E <- df.demog_E %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

```{r demog_M, include = F, cache = T}
df.demog_M <- df.demog %>%
  filter(country == "CN")

df.age_M <- df.demog_M %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_M <- df.demog_M %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

```{r demog_V, include = F, cache = T}
df.demog_V <- df.demog %>%
  filter(country == "VN")

df.age_V <- df.demog_V %>%
  filter(demog_question == "age") %>%
  mutate(demog_response = as.numeric(demog_response)) %>%
  summarize(mean_age = mean(demog_response), 
            median_age = median(demog_response), 
            sd_age = sd(demog_response))

df.gender_V <- df.demog_V %>%
  filter(demog_question == "gender") %>%
  group_by(demog_response) %>%
  rename(gender = demog_response) %>%
  summarize(n=n())
```

## Participants
We recruited `r df.excl_US$ppts_finished` participants from the US, `r df.excl_VN$ppts_finished` participants from Vietnam, and `r df.excl_CN$ppts_finished` participants from mainland China. US participants were recruited through snowball sampling seeded with Stanford student email lists, Vietnam participants were recruited through snowball sampling seeded with Vietnam-based student groups on Facebook, and mainland China participants were recruited through snowball sampling seeded with group chats on WeChat. US participants were compensated with $5 gift certificates (USD), VN participants received 50,000₫ (VND) in phone credit, and mainland China participants received 25CNY through WeChat credit transfer.

We excluded `r df.excl_US$ppts_finished - df.excl_US$ppts_after_att_check_excl` US participants, `r df.excl_VN$ppts_finished - df.excl_VN$ppts_after_att_check_excl` Vietnam participants, and `r df.excl_CN$ppts_finished - df.excl_CN$ppts_after_att_check_excl` China participants who did not answer all attention checks correctly. We followed 4 exclusion criteria that aim to retain only participants who are influenced by one culture: (1) non-native speakers of English and Vietnamese, respectively, (2) fluent in at least one of the other two study languages (Vietnamese for US participants, English for Vietnamese participants and Chinese participants), (3) have lived outside of the test country (US, Vietnam, or China) for more than two years, and (4) have significant international experience (more than 6 international experiences of 2 days or longer.) We did not use a particular criterion for a language if it would exclude 25% or more of any one sample. In this round of exclusion, we excluded `r df.excl_US$ppts_after_att_check_excl - df.excl_US$ppts_after_demog_excl` US, `r df.excl_VN$ppts_after_att_check_excl - df.excl_VN$ppts_after_demog_excl` Vietnam participants, and `r df.excl_CN$ppts_after_att_check_excl - df.excl_CN$ppts_after_demog_excl` China participants. After these exclusions, the US sample included `r df.excl_US$ppts_after_demog_excl` participants (`r df.gender_E %>% filter(gender == "Male") %>% pull(n)`M, `r df.gender_E %>% filter(gender == "Female") %>% pull(n)`F, `r df.gender_E %>% filter(gender == "Non-binary") %>% pull(n)` non-binary, `r df.gender_E %>% filter(gender == "Decline to answer") %>% pull(n)` other), with mean age = `r round(df.age_E$mean_age, 2)` (SD = `r round(df.age_E$sd_age, 2)`) and median age = `r df.age_E$median_age`. The Vietnam sample included `r df.excl_VN$ppts_after_demog_excl` participants (`r df.gender_V %>% filter(gender == "Nam") %>% pull(n)`M, `r df.gender_V %>% filter(gender == "Nữ") %>% pull(n)`F, `r df.gender_V %>% filter(gender == "Từ chối trả lời") %>% pull(n)` other), with mean age = `r round(df.age_V$mean_age, 2)` (SD = `r round(df.age_V$sd_age, 2)`) and median age = `r df.age_V$median_age`. The China sample included `r df.excl_CN$ppts_after_demog_excl` participants (`r df.gender_M %>% filter(gender == "男性") %>% pull(n)`M, `r df.gender_M %>% filter(gender == "女性") %>% pull(n)`F, `r df.gender_M %>% filter(gender == "拒绝回答") %>% pull(n)` other), with mean age = `r round(df.age_M$mean_age, 2)` (SD = `r round(df.age_M$sd_age, 2)`) and median age = `r df.age_M$median_age`.

Notably, we lost a majority of our Vietnam participants (more than 60%) in the attention check exclusion. One reason we suspect why this might have happened is because our Vietnam participants are less familiar with research surveys and attention check questions, and thus might have thought too much about the attention check questions. We carried out an exploratory analysis where we used a less stringent attention check exclusion (covered in the final part of the Results & Analysis section). To preview our findings for this analysis, we did not find any difference in significant results when compared with the main analysis. 

## Stimuli
We adapted stimuli from previous studies to create a set of test triads consisting of a cue, with one thematic and one taxonomic match option. For example, “cow,” “grass,” and “chicken,” where “cow” is the cue, “grass” is the thematic match, and “chicken” the taxonomic match. We included 105 such triads, a superset including triads pulled from supplemental information and in-text examples across the literature, and others that we adapted or created (see SI for the full list of stimuli and sources). We selected triads on the basis of cultural familiarity in the US, Vietnam, and China. The triads were originally in English; they were translated to Vietnamese and Mandarin by a fluent bilingual speaker in each language. The translations were then checked for accuracy after backtranslation to English by another fluent bilingual in each language who was naive to the original English versions.

Each participant completed all 105 triads in sets of 21 trials at a time (10 test triads, 10 filler triads, and 1 attention check per page), by selecting the match most related to the cue (“Which thing is most closely related to the bolded item?” or "Thứ nào liên quan nhất với thứ được in đậm?"). The test triads were presented with 105 filler triads mixed in, to obscure the taxonomic-thematic two-answer forced choice structure of the test stimuli and reduce the likelihood that participants would become aware of the design. The filler triads were groups of three semantically related words, but where the match options were not distinguished by thematic vs. taxonomic similarity, e.g., cue “bird” with match options “lizard” and “toad.” Additionally, we included 10 attention check trials, which were formatted like the test and filler triads but included an instruction instead of a cue item, e.g., “Choose wife” with match options “wife” and “husband.” In total, each participant completed 210 similarity judgments and 10 attention check questions, with triads presented in randomized orders that varied between subjects. 

## Corpus model 
Our general approach is to build a model of similarity that is based on collocation counts in each language. 

### Raw lexical co-occurrences
To give an intuition for our model, consider the cow-grass-chicken triad: we counted how many times “cow” and “grass” co-occur within a window of text in each corpus, and compare this to how many times “cow” and “chicken” co-occur. Our similarity prediction is then proportional to the relative frequency of these pairs. For example, if the thematic cow-grass match accounts for 30% of collocations for this triad (with cow-chicken making up the other 70%), then our model predicts, correspondingly, that 30% of responses to the triad will be grass, and the other 70% chicken. We then use a mixed-effects regression to evaluate how well each corpus collocate model predicts participants’ similarity judgments, across triads and languages.

#### Collocate retrieval and coding
For our English co-occurrence metric, we used the online interface of the Corpus of Contemporary American English [@COCA] to retrieve collocation counts. We recorded the raw count of times that any cue-match pair (e.g., cow-grass or cow-chicken) co-occur in a window of 19 words, which is the maximum window size in the online interface, and closest to the sentence co-occurrence metric in our VI corpus collocation counts. 

To determine Vietnamese co-occurrence, we used the raw frequency of sentence co-occurrences from a subset of the Vietnamese corpus in the Leipzig Corpora Collection [@VICorpus]; this corpus includes 70 million Vietnamese sentences, but our corpus data comes from a 1 million sentence subset for which co-occurrence counts are available for download. Vietnamese makes very frequent use of compositional morphology but the written language  uses spacing to delineate syllable boundaries rather than word boundaries. Accordingly, collocate searches returned instances of both target terms and many morphologically related, but distinct, words. We included in our counts any instances of the target term or close semantic neighbors containing the same morpheme(s) as long as they entailed a likely literal reference to the target term. For example, our search for collocates of the term “gà” (chicken) also returned “gà mái,” a distinct word for female chickens. Despite being a different word, “gà mái” both includes the morpheme for “chicken” and entails reference to a chicken. Accordingly, instances of both “gà” and “gà mái” were included in our collocate count for “chicken.” Some compounds do not meet this criteria, and were excluded from collocate counts. For example “trái cây” (tree.fruits) includes the same syllable as the word “cây” (tree), but refers to fruit that comes from trees, not the trees themselves, and was therefore excluded from our collocate counts for “tree.”

TODO; ADD INFO ABOUT MANDARIN CORPUS

#### Collocate similarity model
From the raw co-occurrence counts of each triad, we calculated the thematic co-occurrence ratio as the number of thematic co-occurrences over the sum of thematic and taxonomic co-occurrences. We did this for both the English and Vietnamese corpus co-occurrence counts. In this way, we obtained predictions for 73 of 105 triads from the Vietnamese corpus and 104 of 105 triads from English. We therefore limited all analyses reported here to the subset of triads for which our corpus-based model can make meaningful predictions, meaning triads that have at least one non-zero collocate count (either thematic or taxonomic match). We replaced any remaining zero collocate counts with $\epsilon$ to account for sparsity in the corpus data. We then tested whether these simple relative frequency models predict US and VN responding.

### Cosine distance of word vectors
To give an intuition for our model, consider again the cow-grass-chicken triad: we retrieved word vectors for "cow" and "grass", and calculate the cosine distance between these vectors. Similarly, we retrieved vectors for "cow" and "chicken" and calculate the cosine distance between them. Our similarity prediction is then inversely proportional to the cosine distance of these pairs. This is because a larger cosine distance means the word vectors are further apart, and thus the words are less similar. For example, if the cosine distance of thematic cow-grass is 0.7 and the cosine distance of taxonomic cow-chicken is 0.3, then our model predicts, correspondingly, that 30% of responses to the triad will be grass, and the other 70% chicken. We then use a mixed-effects regression to evaluate how well each corpus model predicts participants’ similarity judgments, across triads and languages.

#### Collocate retrieval
We use the fastText pre-trained models of English, Mandarin, and Vietnamese in @Grave2018. These models are trained on Common Crawl and Wikipedia using We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. These models were trained using a Continuous Bag of Words (CBOW) with position-weights and a window of size 5. The models use character n-grams of length 5 and 10  negative examples.From the aforementioned models, we retrieve the word vectors (dimension 300) for each word we are interested in.

#### Collocate similarity model
From the word vectors, we calculated the cosine distance between each cue-thematic match (thematic cosine distance) and cue-taxonomic match (taxonomic cosine distance), using the spatial.distance.cosine function from the SciPy package [@2020SciPy]. We then calculated the thematic cosine distance proportion as thematic cosine distance over the sum of taxonomic cosine distance and thematic cosine distance. We did this for all three corpora. We were able to obtain predictions for all triads in all languages.

# Results

## 1. Do we extend previous work reporting a preference for taxonomic matching in the US and thematic matching in Asia?

```{r warning=FALSE, cache = TRUE, include=FALSE, paged.print=TRUE}
df.country <- df %>%
  group_by(subject, country) %>%
  summarize(theme_resp_percent = mean(responses_theme, na.rm = T))

df.country_sum <- df.country %>%
  group_by(country) %>%
  summarize(mean_theme_resp_percent = mean(theme_resp_percent), 
            sd_theme_resp_percent = sd(theme_resp_percent))

fit.country = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df, 
                    family = "binomial")
summary(fit.country) 
fit.country.anova = Anova(fit.country, type=3)

fit.country_EN_VN = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df %>% filter(country != "China"),
                    family = "binomial")
summary(fit.country_EN_VN) 
```
The group means of proportion of thematic response in mainland China is the highest (M = `r round((df.country_sum %>% filter(country == "China"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "China"))$sd_theme_resp_percent, 2)`), followed by the groups means in Vietnam (M = `r round((df.country_sum %>% filter(country == "Vietnam"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "Vietnam"))$sd_theme_resp_percent, 2)`), which is slightly higher than that of the US (M = `r round((df.country_sum %>% filter(country == "US"))$mean_theme_resp_percent, 2)`, SD = `r round((df.country_sum %>% filter(country == "US"))$sd_theme_resp_percent, 2)`).

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.cap="Proportion of thematic responses by country. Only US-China responding comparison shows a siginficant difference. We could not extend this to US-Vietnam responding comparison."}

#show violin plot
ggplot(df.country,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = theme_resp_percent, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_violin() +
  geom_jitter(height = 0, 
              alpha = 0.3) +  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  labs(y = "Proportion Thematic Chosen", 
       x = "Country", 
       color = "Country") + 
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237"))
```

```{r correlation of country response, cache = TRUE, echo=FALSE, warning=FALSE}
#US-China, US-Vietnam, China-Vietnam
df.triad_country <- df %>%
  group_by(triad, country) %>%
  summarize(emp_theme_prop = mean(responses_theme))

df.triad_country <- pivot_wider(data = df.triad_country, 
                                names_from = country, 
                                values_from = emp_theme_prop)
plotUSCN = ggplot(data = df.triad_country,
                  mapping = aes(x = US, 
                                y = China, 
                                label = triad)) +
   geom_text() + 
   geom_smooth(method = "lm") +
   labs(x = "Proportion Thematic Chosen (US)", 
        y = "Proportion Thematic Chosen (China)") + 
   xlim(0, 1) + 
   ylim(0, 1) + 
  theme(axis.title = element_text(size=rel(0.75)))

plotUSVN = ggplot(data = df.triad_country,
                  mapping = aes(x = US, 
                                y = Vietnam, 
                                label = triad)) +
   geom_text() + 
   geom_smooth(method = "lm") +
   labs(x = "Proportion Thematic Chosen (US)", 
        y = "Proportion Thematic Chosen (Vietnam)") + 
   xlim(0, 1) + 
   ylim(0, 1)  + 
  theme(axis.title = element_text(size=rel(0.75)))

plotCNVN = ggplot(data = df.triad_country,
                  mapping = aes(x = Vietnam,
                                y = China, 
                                label = triad)) +
   geom_text() + 
   geom_smooth(method = "lm") +
   labs(x = "Proportion Thematic Chosen (Vietnam)", 
        y = "Proportion Thematic Chosen (China)") + 
   xlim(0, 1) + 
   ylim(0, 1)  + 
  theme(axis.title = element_text(size=rel(0.75)))

corr.USCN = apa_print(cor.test(df.triad_country$US, 
                  df.triad_country$China, na.rm=T))

corr.USVN = apa_print(cor.test(df.triad_country$US, 
                  df.triad_country$Vietnam, na.rm=T))

corr.CNVN = apa_print(cor.test(df.triad_country$China, 
                  df.triad_country$Vietnam, na.rm=T))
```

<!-- With a Pearson correlation test, we observed high correlations between thematic responding across all three countries. The proportion of US thematic responding is strongly correlated with thematic responding in China, `r corr.USCN$full_result`. The proportion of US thematic responding is strongly correlated with thematic responding in Vietnam, `r corr.USVN$full_result`. Likewise, the proportion of China thematic response is strongly correlated with thematic responding in Vietnam, `r corr.CNVN$full_result`. This points to a strong signal of universality across cultural contexts regarding similarity judgments. -->

```{r USCN effect size comparison, cache = TRUE, include=FALSE}
USCN_cohen <- cohen.d(df.triad_country$China, df.triad_country$US)
```

To test for cross-context differences in similarity judgments between the countries, we ran a mixed-effects logistic regression predicting triad responding (taxonomic or thematic) with country (US, China, or Vietnam) as a fixed effect. As random effects, we included an intercept per subject and one per triad, as well as by-triad random slopes for country to account for variation in the country effect across triads. In R syntax, the model is: response ~ country + (1 | subject) + (country | triad).

Overall, there is a significant effect of country on proportion of thematic responses ($\chi^2$(`r fit.country.anova["country", "Df"]`) = `r round(fit.country.anova["country", "Chisq"], 2)`, p `r ifelse( fit.country.anova["country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.country.anova["country", "Pr(>Chisq)"], 3)), "< .001")`). However, this effect is driven by the difference between US and China responding ($\beta$ = `r round(fixef(fit.country)["countryUS"], 2)`, p `r ifelse(as.data.frame(summary(fit.country)[["coefficients"]])["countryUS", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country)[["coefficients"]])["countryUS", "Pr(>|z|)"], 3)), "< .001")`). There is no statistical difference between the Vietnam and China responding ($\beta$ = `r round(fixef(fit.country)["countryVietnam"], 2)`, p `r ifelse(as.data.frame(summary(fit.country)[["coefficients"]])["countryVietnam", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country)[["coefficients"]])["countryVietnam", "Pr(>|z|)"], 3)), "< .001")`), and the US and Vietnam responding ($\beta$ = `r round(fixef(fit.country_EN_VN)["countryVietnam"], 2)`, p `r ifelse(as.data.frame(summary(fit.country_EN_VN)[["coefficients"]])["countryVietnam", "Pr(>|z|)"] > 0.001, paste0("= ", round(as.data.frame(summary(fit.country_EN_VN)[["coefficients"]])["countryVietnam", "Pr(>|z|)"], 3)), "< .001")`).

On this analysis, we do not find support that the US-China tendencies toward taxonomic and thematic responding (respectively) extend to the US-Vietnam comparison. Accordingly, we cannot speak to overall biases toward thematic responding across Asian cultural contexts broadly, but we do replicate the differences documented by @Ji2004 between the US and China. However, in our corpus model comparison, we do find evidence for different, more fine-grained variation in similarity judgments between the US and Vietnam. 

## 2. Can the differences in similarity judgments between English, Mandarin and Vietnamese speakers be explained by variation in lexical statistics?
### Is responding in each cultural context predicted by the corresponding lexical statistics (raw co-occurrences)? 
#### Raw lexical co-occurrences
```{r Pearson correlation between raw co-occurrences of corpora and responding, warning=FALSE, eval = F, cache = TRUE, include=FALSE}

#raw co-occurrences
df.triad_country_freq <- df %>%
  filter(!(triad %in% triads_omit)) %>% # omit triads that do not have meaningful prediction (at least 1 non-zero prediction in both languages)
  group_by(triad, country, theme_freq_prop_E, theme_freq_prop_V, theme_freq_prop_M) %>%  # not working 
  summarize(emp_theme_prop = mean(responses_theme))
df.triad_country_freq   #sanity

df.triad_country_freq_E <- df.triad_country_freq %>%
  filter(country == "US")

#plotting English corpus against US responding data
plotE = ggplot(data = df.triad_country_freq_E,
        mapping = aes(x = emp_theme_prop,
                     y = theme_freq_prop_E,
                     label = triad)) +
   geom_text() +
   geom_smooth(method = "lm") +
   labs(x = "Proportion Thematic Chosen (US)",
        y = "Proportion Thematic Predicted (English)")
plotE

df.triad_country_freq_V <- df.triad_country_freq %>%
  filter(country == "Vietnam")

#plotting Vietnamese corpus against VN responding data
plotV = ggplot(data = df.triad_country_freq_V,
        mapping = aes(x = emp_theme_prop,
                     y = theme_freq_prop_V,
                     label = triad)) +
   geom_text() +
   geom_smooth(method = "lm") +
   labs(x = "Proportion Thematic Chosen (Vietnam)",
        y = "Proportion Thematic Predicted (Vietnamese)")
plotV

df.triad_country_freq_M <- df.triad_country_freq %>%
  filter(country == "China")

#plotting China corpus against CN responding data
plotM = ggplot(data = df.triad_country_freq_M,
        mapping = aes(x = emp_theme_prop,
                     y = theme_freq_prop_M,
                     label = triad)) +
   geom_text() +
   geom_smooth(method = "lm") +
   labs(x = "Proportion Thematic Chosen (China)",
        y = "Proportion Thematic Predicted (Chinese)")
plotM

#Pearson correlation between corpus and responding
corr.E = apa_print(cor.test(df.triad_country_freq_E$emp_theme_prop,
                  df.triad_country_freq_E$theme_freq_prop_E, na.rm=T))
corr.E

corr.V = apa_print(cor.test(df.triad_country_freq_V$emp_theme_prop,
                  df.triad_country_freq_V$theme_freq_prop_V, na.rm=T))
corr.V

corr.M = apa_print(cor.test(df.triad_country_freq_M$emp_theme_prop,
                  df.triad_country_freq_M$theme_freq_prop_M, na.rm=T))
corr.M

#Pearson correlation between EN and VI corpus
corr.EV = apa_print(cor.test(df.triad_country_freq_V$theme_freq_prop_V,
                   df.triad_country_freq_E$theme_freq_prop_E, na.rm=T))
corr.EM = apa_print(cor.test(df.triad_country_freq_M$theme_freq_prop_M,
                   df.triad_country_freq_E$theme_freq_prop_E, na.rm=T))
corr.MV = apa_print(cor.test(df.triad_country_freq_V$theme_freq_prop_V,
                   df.triad_country_freq_M$theme_freq_prop_M, na.rm=T))
```

```{r echo=FALSE, warning = F, eval = F, cache = TRUE, paged.print=FALSE, fig.cap ="Correlation between English corpus lexical co-occurrence and US responding, and Vietnamese corpus lexical co-occurrence and Vietnam responding."}
plotE + plotV + plotM
  plot_layout(ncol = 3)
```

To test whether variation in lexical co-occurrence can explain differences in similarity judgments between US and Vietnam participants, we compare logistic mixed-effects regression models fit to the thematic responding data from each country separately. We first ask how well each corpus model (English, Vietnamese, or Mandarin) predicts similarity judgments by speakers of the corresponding language (US, Vietnam, or China). To do this, we use a mixed-effects logistic regression to predict triad responses (0=taxonomic or 1=thematic) with corpus prediction (proportion thematic co-occurrence or cosine distance) as a fixed effect and participant and triad as random effects.

We found that all corpora are significant predictors of all cultural context responding.
```{r US responding predicted by each corpus model raw co-occurrences, warning=FALSE, include=F, cache = TRUE}
# English 
fit.EN_US = glmer(responses_theme ~ theme_freq_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.EN_US)
fit.EN_US.anova = Anova(fit.EN_US, type = 3)

#Vietnamese
fit.VI_US = glmer(responses_theme ~ theme_freq_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.VI_US)
fit.VI_US.anova = Anova(fit.VI_US, type = 3)

#Mandarin
fit.ZH_US = glmer(responses_theme ~ theme_freq_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.ZH_US)
fit.ZH_US.anova = Anova(fit.ZH_US, type = 3)
```
For US responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.EN_US)["theme_freq_prop_E"], 2)`, $\chi^2$(`r fit.EN_US.anova["theme_freq_prop_E", "Df"]`) = `r round(fit.EN_US.anova["theme_freq_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_US.anova["theme_freq_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_US.anova["theme_freq_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.VI_US)["theme_freq_prop_V"], 2)`, $\chi^2$(`r fit.VI_US.anova["theme_freq_prop_V", "Df"]`) = `r round(fit.VI_US.anova["theme_freq_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_US.anova["theme_freq_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_US.anova["theme_freq_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ZH_US)["theme_freq_prop_M"], 2)`, $\chi^2$(`r fit.ZH_US.anova["theme_freq_prop_M", "Df"]`) = `r round(fit.ZH_US.anova["theme_freq_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_US.anova["theme_freq_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_US.anova["theme_freq_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

```{r VN responding predicted by each corpus model raw co-occurrences, warning=FALSE, include=F, cache = TRUE}
# English 
fit.EN_VN = glmer(responses_theme ~ theme_freq_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.EN_VN)
fit.EN_VN.anova = Anova(fit.EN_VN, type = 3)

#Vietnamese
fit.VI_VN = glmer(responses_theme ~ theme_freq_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.VI_VN)
fit.VI_VN.anova = Anova(fit.VI_VN, type = 3)

#Mandarin
fit.ZH_VN = glmer(responses_theme ~ theme_freq_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.ZH_VN)
fit.ZH_VN.anova = Anova(fit.ZH_VN, type = 3)
```
For Vietnam responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.EN_VN)["theme_freq_prop_E"], 2)`, $\chi^2$(`r fit.EN_VN.anova["theme_freq_prop_E", "Df"]`) = `r round(fit.EN_VN.anova["theme_freq_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_VN.anova["theme_freq_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_VN.anova["theme_freq_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.VI_VN)["theme_freq_prop_V"], 2)`, $\chi^2$(`r fit.VI_VN.anova["theme_freq_prop_V", "Df"]`) = `r round(fit.VI_VN.anova["theme_freq_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_VN.anova["theme_freq_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_VN.anova["theme_freq_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ZH_VN)["theme_freq_prop_M"], 2)`, $\chi^2$(`r fit.ZH_VN.anova["theme_freq_prop_M", "Df"]`) = `r round(fit.ZH_VN.anova["theme_freq_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_VN.anova["theme_freq_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_VN.anova["theme_freq_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

```{r CN responding predicted by each corpus model raw co-occurrences, warning=FALSE, include=F, cache = TRUE}
#English
fit.EN_CN = glmer(responses_theme ~ theme_freq_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.EN_CN)
fit.EN_CN.anova = Anova(fit.EN_CN, type = 3)

#Vietnamese
fit.VI_CN = glmer(responses_theme ~ theme_freq_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.VI_CN)
fit.VI_CN.anova = Anova(fit.VI_CN, type = 3)

#Mandarin
fit.ZH_CN = glmer(responses_theme ~ theme_freq_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China", !(triad %in% triads_omit)), 
          family="binomial")
summary(fit.ZH_CN)
fit.ZH_CN.anova = Anova(fit.ZH_CN, type = 3)
```

For China responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.EN_CN)["theme_freq_prop_E"], 2)`, $\chi^2$(`r fit.EN_CN.anova["theme_freq_prop_E", "Df"]`) = `r round(fit.EN_CN.anova["theme_freq_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_CN.anova["theme_freq_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_CN.anova["theme_freq_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.VI_CN)["theme_freq_prop_V"], 2)`, $\chi^2$(`r fit.VI_CN.anova["theme_freq_prop_V", "Df"]`) = `r round(fit.VI_CN.anova["theme_freq_prop_V", "Chisq"], 2)`, p `r ifelse( fit.VI_CN.anova["theme_freq_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_CN.anova["theme_freq_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ZH_US)["theme_freq_prop_M"], 2)`, $\chi^2$(`r fit.ZH_CN.anova["theme_freq_prop_M", "Df"]`) = `r round(fit.ZH_CN.anova["theme_freq_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_CN.anova["theme_freq_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_CN.anova["theme_freq_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

This suggests a strong shared similarity signal across the languages.

#### Cosine distances of fastText word vectors
Similar to the results from the analysis with raw lexical co-occurrences, we found that all corpora are significant predictors of all cultural context responding. 

```{r US responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
#English
fit.EN_US_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.EN_US_cos)
fit.EN_US_cos.anova = Anova(fit.EN_US_cos, type = 3)
fit.EN_US_cos.anova

#Vietnamese
fit.VI_US_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.VI_US_cos)
fit.VI_US_cos.anova = Anova(fit.VI_US_cos, type = 3)
fit.VI_US_cos.anova

#Mandarin
fit.ZH_US_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "US"), 
          family="binomial")
summary(fit.ZH_US_cos)
fit.ZH_US_cos.anova = Anova(fit.ZH_US_cos, type = 3)
fit.ZH_US_cos.anova
```
For US responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are significant predictors of US responding (EN-US model: $\beta$ = `r round(fixef(fit.EN_US_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_US_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.EN_US_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`; VI-US model: $\beta$ = `r round(fixef(fit.VI_US_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_US_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.VI_US_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p = `r ifelse( fit.VI_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`; ZH-US model: $\beta$ = `r round(fixef(fit.ZH_US_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_US_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ZH_US_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`). 

```{r VN responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
#English
fit.EN_VN_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.EN_VN_cos)
fit.EN_VN_cos.anova = Anova(fit.EN_VN_cos, type = 3)
fit.EN_VN_cos.anova

#Vietnamese
fit.VI_VN_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.VI_VN_cos)
fit.VI_VN_cos.anova = Anova(fit.VI_VN_cos, type = 3)
fit.VI_VN_cos.anova

#Mandarin
fit.ZH_VN_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.ZH_VN_cos)
fit.ZH_VN_cos.anova = Anova(fit.ZH_VN_cos, type = 3)
fit.ZH_VN_cos.anova
```

For Vietnam (VN) responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are significant predictors of VN responding (EN-VN model: $\beta$ = `r round(fixef(fit.EN_VN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_VN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.EN_VN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`; VI-VN model: $\beta$ = `r round(fixef(fit.VI_VN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_VN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.VI_VN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p = `r ifelse( fit.VI_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`; ZH-VN model: $\beta$ = `r round(fixef(fit.ZH_VN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`). 

```{r CN responding predicted by each corpus model cosine differences, warning=FALSE, include=F}
## English
fit.EN_CN_cos = glmer(responses_theme ~ theme_cosine_prop_E + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.EN_CN_cos)
fit.EN_CN_cos.anova = Anova(fit.EN_CN_cos, type = 3)
fit.EN_CN_cos.anova

#Vietnamese
fit.VI_CN_cos = glmer(responses_theme ~ theme_cosine_prop_V + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.VI_CN_cos)
fit.VI_CN_cos.anova = Anova(fit.VI_CN_cos, type = 3)
fit.VI_CN_cos.anova

#Mandarin
fit.ZH_CN_cos = glmer(responses_theme ~ theme_cosine_prop_M + (1 | subject) + (1 | triad), 
          data = df %>% filter(country == "China"), 
          family="binomial")
summary(fit.ZH_CN_cos)
fit.ZH_CN_cos.anova = Anova(fit.ZH_CN_cos, type = 3)
fit.ZH_CN_cos.anova
```

For China (CN) responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are significant predictors of CN responding (EN-CN model: $\beta$ = `r round(fixef(fit.EN_CN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.EN_CN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.EN_CN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.EN_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.EN_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`; VI-CN model: $\beta$ = `r round(fixef(fit.VI_CN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.VI_CN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.VI_CN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p = `r ifelse( fit.VI_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VI_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`; ZH-CN model: $\beta$ = `r round(fixef(fit.ZH_CN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`). 

### Is responding in each cultural context __best__ predicted by the corresponding corpus's lexical statistics (raw co-occurrences), as opposed to the other two corpora?
Next, we directly compare the corpus models by including both as fixed effects in three mixed-effect regressions (predicting US, Vietnam and China responding) with the same random effects as above. In R syntax, the model is: response ~ corpus_English + corpus_Vietnamese + corpus_Mandarin + (1|triad) + (1|subject).

#### Raw co-occurrences 
When we included all corpora thematic cosine distance proportion as predictors, all corpora are significant in predicting all cultural context's responding. 

```{r Eng-Vie-Mand lexical co-occ predicting US, warning=FALSE, include=F, cache = TRUE}
# ADDITION: adding theme_freq_prop_M, 
#raw co-occurrences
#linear mixed model with ALL corpora as predictors for each population.
fit.ENVIZH_US = glmer(responses_theme ~ 
                      theme_freq_prop_E + theme_freq_prop_V + theme_freq_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "US", !(triad %in% triads_omit)),
                    family="binomial")
summary(fit.ENVIZH_US)
fit.ENVIZH_US.anova = Anova(fit.ENVIZH_US, type = 3)
fit.ENVIZH_US.anova
```
For US responding: only English (EN) corpus is a significant predictor. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US)["theme_freq_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_US.anova["theme_freq_prop_E", "Df"]`) = `r round(fit.ENVIZH_US.anova["theme_freq_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US.anova["theme_freq_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US.anova["theme_freq_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US)["theme_freq_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_US.anova["theme_freq_prop_V", "Df"]`) = `r round(fit.ENVIZH_US.anova["theme_freq_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US.anova["theme_freq_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US.anova["theme_freq_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US)["theme_freq_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_US.anova["theme_freq_prop_M", "Df"]`) = `r round(fit.ENVIZH_US.anova["theme_freq_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US.anova["theme_freq_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US.anova["theme_freq_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

```{r Eng-Vie-Mand lexical co-occ predicting VN, warning=FALSE, include=F, cache = TRUE}
fit.ENVIZH_VN = glmer(responses_theme ~ 
                      theme_freq_prop_V + theme_freq_prop_E + theme_freq_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "Vietnam", !(triad %in% triads_omit)),
                    family="binomial")
summary(fit.ENVIZH_VN)
fit.ENVIZH_VN.anova = Anova(fit.ENVIZH_VN, type = 3)
fit.ENVIZH_VN.anova
```
For Vietnam responding: only English (EN) and Mandarin (ZH) corpora are significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN)["theme_freq_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_VN.anova["theme_freq_prop_E", "Df"]`) = `r round(fit.ENVIZH_VN.anova["theme_freq_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN.anova["theme_freq_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN.anova["theme_freq_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN)["theme_freq_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_VN.anova["theme_freq_prop_M", "Df"]`) = `r round(fit.ENVIZH_VN.anova["theme_freq_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN.anova["theme_freq_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN.anova["theme_freq_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN)["theme_freq_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_VN.anova["theme_freq_prop_V", "Df"]`) = `r round(fit.ENVIZH_VN.anova["theme_freq_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_VN.anova["theme_freq_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN.anova["theme_freq_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. 

```{r Eng-Vie-Mand lexical co-occ predicting CN, warning=FALSE, include=F, cache = TRUE}
fit.ENVIZH_CN = glmer(responses_theme ~ 
                      theme_freq_prop_V + theme_freq_prop_E + theme_freq_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "China", !(triad %in% triads_omit)),
                    family="binomial")
summary(fit.ENVIZH_CN)
fit.ENVIZH_CN.anova = Anova(fit.ENVIZH_CN, type = 3)
fit.ENVIZH_CN.anova
```
For China responding: only Mandarin (ZH) corpus is a significant predictor. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN)["theme_freq_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_CN.anova["theme_freq_prop_M", "Df"]`) = `r round(fit.ENVIZH_CN.anova["theme_freq_prop_M", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN.anova["theme_freq_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN.anova["theme_freq_prop_M", "Pr(>Chisq)"], 3)), "< .001")`. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN)["theme_freq_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_CN.anova["theme_freq_prop_E", "Df"]`) = `r round(fit.ENVIZH_CN.anova["theme_freq_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN.anova["theme_freq_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN.anova["theme_freq_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN)["theme_freq_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_CN.anova["theme_freq_prop_V", "Df"]`) = `r round(fit.ENVIZH_CN.anova["theme_freq_prop_V", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_CN.anova["theme_freq_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN.anova["theme_freq_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. 

```{r echo=FALSE, warning = FALSE, cache = TRUE, fig.cap = "Fixed effect sizes of each corpus when included as a predictor for China, US, and Vietnam responding, respectively. The English corpus is the best predictor for US response, and the Mandarin corpus is the best predictor for China response."}
coeff_value_freq <- c(fixef(fit.ENVIZH_US)["theme_freq_prop_M"],
                 fixef(fit.ENVIZH_US)["theme_freq_prop_E"], 
                 fixef(fit.ENVIZH_US)["theme_freq_prop_V"],
                 fixef(fit.ENVIZH_VN)["theme_freq_prop_M"],
                 fixef(fit.ENVIZH_VN)["theme_freq_prop_E"],
                 fixef(fit.ENVIZH_VN)["theme_freq_prop_V"], 
                 fixef(fit.ENVIZH_CN)["theme_freq_prop_M"],
                 fixef(fit.ENVIZH_CN)["theme_freq_prop_E"], 
                 fixef(fit.ENVIZH_CN)["theme_freq_prop_V"])

se_value_freq <- c(summary(fit.ENVIZH_US)$coef["theme_freq_prop_M", "Std. Error"], 
              summary(fit.ENVIZH_US)$coef["theme_freq_prop_E", "Std. Error"],
              summary(fit.ENVIZH_US)$coef["theme_freq_prop_V", "Std. Error"], 
              summary(fit.ENVIZH_VN)$coef["theme_freq_prop_M", "Std. Error"], 
              summary(fit.ENVIZH_VN)$coef["theme_freq_prop_E", "Std. Error"],
              summary(fit.ENVIZH_VN)$coef["theme_freq_prop_V", "Std. Error"],
              summary(fit.ENVIZH_CN)$coef["theme_freq_prop_M", "Std. Error"], 
              summary(fit.ENVIZH_CN)$coef["theme_freq_prop_E", "Std. Error"],
              summary(fit.ENVIZH_CN)$coef["theme_freq_prop_V", "Std. Error"])

country_value <- c(rep("US", 3),
                   rep("VN", 3),
                   rep("CN", 3))

corpus_value <- c(rep(c("ZH", "EN", "VI"), 3))

df.USVNCN_coeffs_freq <- data.frame(country_value, corpus_value, coeff_value_freq, se_value_freq) %>%
  mutate(corpus_value = factor(corpus_value, levels=c("ZH", "EN", "VI")))

ggplot(data = df.USVNCN_coeffs_freq, 
       mapping = aes(x = country_value, y = coeff_value_freq, fill = corpus_value)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin= coeff_value_freq - se_value_freq, ymax = coeff_value_freq + se_value_freq), width=.2,
                 position=position_dodge(.9)) +
  labs(x = "Country", y = "Fixed effect size", fill = "Corpus") + 
  scale_x_discrete(labels=c("CN" = "China", "US" = "US", "VN" = "Vietnam")) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237"))
  
```

```{r freq model comparison, include=FALSE, cache = TRUE}
fit.US_freq_compare = anova(fit.ENVIZH_US, fit.EN_US, type = 3)
fit.VN_freq_compare = anova(fit.ENVIZH_VN, fit.VI_VN, type = 3)
fit.CN_freq_compare = anova(fit.ENVIZH_CN, fit.ZH_CN, type = 3)
```

We observed some level of language specificity from this analysis. The English corpus is the best predictor for US responding, and the Mandarin corpus is the best predictor for China response. While this is not the case with the Vietnamese corpus and the Vietnam responding, the Vietnamese corpus is still a significant predictor for the Vietnam responding. 

However, we found that language specificity alone does not explain our results. We used an ANOVA to compare the model with only the corresponding corpus, and the model with all 3 corpora for each cultural context. We found that in all three cases (US, China, Vietnam), adding the other two corpora produces a significantly better fit than the identical model without the additional corpora, and only the corresponding corpus included as a predictor (US response: $\chi^2$(`r fit.US_freq_compare["fit.ENVIZH_US", "Df"]`) = `r round(fit.US_freq_compare["fit.ENVIZH_US", "Chisq"], 2)`, p `r ifelse( fit.US_freq_compare["fit.ENVIZH_US", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.US_freq_compare["fit.ENVIZH_US", "Pr(>Chisq)"], 3)), "< .001")`; Vietnam response: $\chi^2$(`r fit.VN_freq_compare["fit.ENVIZH_VN", "Df"]`) = `r round(fit.VN_freq_compare["fit.ENVIZH_VN", "Chisq"], 2)`, p `r ifelse( fit.VN_freq_compare["fit.ENVIZH_VN", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VN_freq_compare["fit.ENVIZH_VN", "Pr(>Chisq)"], 3)), "< .001")`; China response: $\chi^2$(`r fit.CN_freq_compare["fit.ENVIZH_CN", "Df"]`) = `r round(fit.CN_freq_compare["fit.ENVIZH_CN", "Chisq"], 2)`, p `r ifelse( fit.CN_freq_compare["fit.ENVIZH_CN", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.CN_freq_compare["fit.ENVIZH_CN", "Pr(>Chisq)"], 3)), "< .001")`).  

#### Cosine distances of fastText word vectors

```{r Eng-Vie-Mand cosine similarities predicting US-VN-CN, warning=FALSE, include=FALSE}
## English/Vietnamese/Mandarin comparison
fit.ENVIZH_US_cos = glmer(responses_theme ~ theme_cosine_prop_E + theme_cosine_prop_V + theme_cosine_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "US"), 
                    family="binomial")
summary(fit.ENVIZH_US_cos)
fit.ENVIZH_US_cos.anova = Anova(fit.ENVIZH_US_cos, type = 3)
fit.ENVIZH_US_cos.anova

fit.ENVIZH_VN_cos = glmer(responses_theme ~ theme_cosine_prop_V + theme_cosine_prop_E + theme_cosine_prop_M +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "Vietnam"), 
                    family="binomial")
summary(fit.ENVIZH_VN_cos)
fit.ENVIZH_VN_cos.anova = Anova(fit.ENVIZH_VN_cos, type = 3)
fit.ENVIZH_VN_cos.anova

fit.ENVIZH_CN_cos = glmer(responses_theme ~ theme_cosine_prop_M + theme_cosine_prop_E + theme_cosine_prop_V +
                      (1 | subject) + (1 | triad), 
                    data = df %>% filter(country == "China"), 
                    family="binomial")
summary(fit.ENVIZH_CN_cos)
fit.ENVIZH_CN_cos.anova = Anova(fit.ENVIZH_CN_cos, type = 3)
fit.ENVIZH_CN_cos.anova
```

When we included all corpora thematic cosine distance proportion as predictors, all corpora are significant in predicting all cultural context's responding. 

For US responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_US_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

For Vietnam responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_VN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

For China responding: English (EN), Vietnamese (VI), and Mandarin (ZH) corpus are all significant predictors. EN corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_E"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_E", "Pr(>Chisq)"], 3)), "< .001")`. VI corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_V"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_V", "Pr(>Chisq)"], 3)), "< .001")`. ZH corpus: $\beta$ = `r round(fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_M"], 2)`, $\chi^2$(`r fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Df"]`) = `r round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Chisq"], 2)`, p = `r ifelse( fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.ENVIZH_CN_cos.anova["theme_cosine_prop_M", "Pr(>Chisq)"], 3)), "< .001")`.

We observed some level of language specificity from this analysis. The English corpus is the best predictor for US responding, and the Mandarin corpus is the best predictor for China response. While this is not the case with the Vietnamese corpus and the Vietnam responding, the Vietnamese corpus is still a significant predictor for the Vietnam responding. 

```{r echo=FALSE, warning = FALSE, fig.cap = "Fixed effect sizes of each corpus when included as a predictor for China, US, and Vietnam responding, respectively. The English corpus is the best predictor for US response, and the Mandarin corpus is the best predictor for China response."}
coeff_value_cos <- c(fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_V"], 
                 fixef(fit.ENVIZH_US_cos)["theme_cosine_prop_M"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_V"],
                 fixef(fit.ENVIZH_VN_cos)["theme_cosine_prop_M"], 
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_E"],
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_V"], 
                 fixef(fit.ENVIZH_CN_cos)["theme_cosine_prop_M"])

se_value_cos <- c(summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_US_cos)$coef["theme_cosine_prop_M", "Std. Error"], 
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_VN_cos)$coef["theme_cosine_prop_M", "Std. Error"],
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_E", "Std. Error"], 
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_V", "Std. Error"],
              summary(fit.ENVIZH_CN_cos)$coef["theme_cosine_prop_M", "Std. Error"])

country_value <- c(rep("US", 3),
                   rep("VN", 3),
                   rep("CN", 3))

corpus_value <- c(rep(c("EN", "VI", "ZH"), 3))

df.USVNCN_coeffs_cos <- data.frame(country_value, corpus_value, coeff_value_freq, se_value_freq) %>%
  mutate(corpus_value = factor(corpus_value, levels=c("ZH", "EN", "VI")))

ggplot(data = df.USVNCN_coeffs_cos, 
       mapping = aes(x = country_value, y = coeff_value_cos, fill = corpus_value)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin= coeff_value_cos - se_value_cos, ymax = coeff_value_cos + se_value_cos), width=.2,
                 position=position_dodge(.9)) +
  scale_y_reverse() + 
  labs(x = "Country", y = "Fixed effect size", fill = "Corpus") + 
  scale_x_discrete(labels=c("CN" = "China", "US" = "US", "VN" = "Vietnam")) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237"))
  
```

```{r model comparison, include=FALSE}
fit.US_cos_compare = anova(fit.ENVIZH_US_cos, fit.EN_US_cos, type = 3)
fit.VN_cos_compare = anova(fit.ENVIZH_VN_cos, fit.VI_VN_cos, type = 3)
fit.CN_cos_compare = anova(fit.ENVIZH_CN_cos, fit.ZH_CN_cos, type = 3)
```

However, we found that language specificity alone does not explain our results. We used an ANOVA to compare the model with only the corresponding corpus, and the model with all 3 corpora for each cultural context. We found that in all three cases (US, China, Vietnam), adding the other two corpora produces a significantly better fit than the identical model without the additional corpora, and only the corresponding corpus included as a predictor (US response: $\chi^2$(`r fit.US_cos_compare["fit.ENVIZH_US_cos", "Df"]`) = `r round(fit.US_cos_compare["fit.ENVIZH_US_cos", "Chisq"], 2)`, p = `r ifelse( fit.US_cos_compare["fit.ENVIZH_US_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.US_cos_compare["fit.ENVIZH_US_cos", "Pr(>Chisq)"], 3)), "< .001")`; Vietnam response: $\chi^2$(`r fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Df"]`) = `r round(fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Chisq"], 2)`, p = `r ifelse( fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.VN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"], 3)), "< .001")`; China response: $\chi^2$(`r fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Df"]`) = `r round(fit.CN_cos_compare["fit.ENVIZH_CN_cos", "Chisq"], 2)`, p = `r ifelse( fit.CN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.CN_cos_compare["fit.ENVIZH_VN_cos", "Pr(>Chisq)"], 3)), "< .001")`). 

```{r add linguistic model to Ji et al., include=FALSE, eval = F}
# this is probably similar to Q3
fit.country = glmer(responses_theme ~ country + (1 | subject) + (1 | triad), 
                           data = df %>% filter(country != "Vietnam"), 
                           family = "binomial")

fit.country_corpus = glmer(responses_theme ~ country + theme_cosine_prop_E + theme_cosine_prop_M + (1 | subject) + (1 | triad), 
                           data = df %>% filter(country != "Vietnam"), 
                           family = "binomial")

fit.add_corpus <- anova(fit.country, fit.country_corpus, type = 3)
                   
```
_TODO: should we take this out?__
We carried out an ANOVA model comparison to compare our approach with previous studies that did not include corpus information. Because @Ji2004 only compared US and China participants, we filtered our data to only include responding data from these two countries. Our approach would be to compare a model that only includes country (US or China) as a fixed effect with one that also includes English and Chinese corpus data. Our previous model with an intercept per subject and one per triad, as well as by-triad random slopes for country to account for variation in the country effect across triads fails to converge with the more complex model, so we include a simpler random effect structure with an intercept per subject and one per triad in both the basic and more complex model. We compare this model with an identical model but including Chinese and English corpus cosine distance proportion prediction. We found that adding English and Chinese corpus data produces a significantly better fit than the identical model without English and Chinese corpus data as predictors ($\chi^2$(`r fit.add_corpus["fit.country_corpus", "Df"]`) = `r round(fit.add_corpus["fit.country_corpus", "Chisq"], 2)`, p = < .001. This suggests that including corpus data would better model the cross-cultural differences in similarity judgment in this study as well as similar studies.

## 3. Does similarity reasoning differ across cultures, only the input to it, or both?
Does adding country and interaction term with the corresponding corpus improves the model? 

```{r, include = F}
df <- df %>%
  mutate(theme_freq_prop_corr_lang = case_when(
    country == "US" ~ theme_freq_prop_E,
    country == "China" ~ theme_freq_prop_M, 
    country == "Vietnam" ~ theme_freq_prop_V)) %>% 
    mutate(theme_cosine_prop_corr_lang = case_when(
    country == "US" ~ theme_cosine_prop_E,
    country == "China" ~ theme_cosine_prop_M, 
    country == "Vietnam" ~ theme_cosine_prop_V))
```

```{r country language interaction in raw co-occurrences, include = F}
fit.language = glmer(responses_theme ~ theme_freq_prop_corr_lang + (1 | subject) + (1 | triad), 
          data = df %>% filter(!(triad %in% triads_omit)),  
          family="binomial")
summary(fit.language)
fit.language.anova = Anova(fit.language, type = 3)
fit.language.anova

fit.country_language = glmer(responses_theme ~ theme_freq_prop_corr_lang * country + (1 | subject) + (1 | triad), 
          data = df %>% filter(!(triad %in% triads_omit)),  
          family="binomial")
summary(fit.country_language)
fit.country_language.anova = Anova(fit.country_language, type = 3)
fit.country_language.anova

fit.country_language_compare = anova(fit.language, fit.country_language, type = 3)
fit.country_language_compare
```

```{r country language interaction in cosine prop, include = F}
fit.language_cos = glmer(responses_theme ~ theme_cosine_prop_corr_lang + (1 | subject) + (1 | triad), 
          data = df,  
          family="binomial")
summary(fit.language_cos)
fit.language_cos.anova = Anova(fit.language_cos, type = 3)
fit.language_cos.anova

fit.country_language_cos= glmer(responses_theme ~ theme_cosine_prop_corr_lang * country + (1 | subject) + (1 | triad), 
          data = df,  
          family="binomial")
summary(fit.country_language_cos)
fit.country_language_cos.anova = Anova(fit.country_language_cos, type = 3)
fit.country_language_cos.anova

fit.country_language_cos_compare = anova(fit.language_cos, fit.country_language_cos, type = 3)
fit.country_language_cos_compare
```

Adding country improves the model. Only country and interaction between country and corresponding corpus's lexical co-occurrence are significant predictors. Corresponding corpus's lexical co-occurrence is no longer a significant predictor.


## 5. How general are the cross-language differences in similarity judgments we observe? Are they limited to taxonomic-thematic contrasts or do they extend to other cases?

```{r incorporate base freq for fillers}
df.filler <- read.csv("../../../data/data_filler_USCNVN_ENZHVI.csv")
fillers_omit <- read.csv("../../../data/fillers_omit.csv") %>%
  pull(x)
```

```{r warning=FALSE, cache = TRUE, include = FALSE, paged.print=TRUE}
df.country_filler <- df.filler %>%
  group_by(subject, country) %>%
  summarize(word1_resp_percent = mean(responses_word1, na.rm = T))
``` 

```{r echo=FALSE, warning=FALSE, cache = TRUE, fig.cap="Proportion of thematic responses by country. Only US-China responding comparison shows a siginficant difference. We could not extend this to US-Vietnam responding comparison."}

#show violin plot
ggplot(df.country_filler,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = word1_resp_percent, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_violin() +
  geom_jitter(height = 0, 
              alpha = 0.3) +  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  labs(y = "Proportion Word1 Chosen", 
       x = "Country", 
       color = "Country") + 
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237"))
```

```{r warning=FALSE, include = F, cache = TRUE, paged.print=TRUE}
# df.country_sum <- df.country %>%
#   group_by(country) %>%
#   summarize(mean_theme_resp_percent = mean(theme_resp_percent), 
#             sd_theme_resp_percent = sd(theme_resp_percent))

fit.country_filler = glmer(responses_word1 ~ country + (1 | subject) + (country | cue),
                    data = df.filler, 
                    family = "binomial")
summary(fit.country_filler) 
fit.country_filler.anova = Anova(fit.country_filler, type=3)
fit.country_filler.anova
``` 
No effect of country on filler responding.

### Does lexical statistics for fillers predict responding in the corresponding cultural context?

#### Raw co-occurrences
```{r US responding predicted by each corpus model raw co-occurrences for fillers, warning=FALSE,include = F}
# English 
fit.EN_US_filler = glmer(responses_word1 ~ word1_match_frequency_prop_E + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "US", 
                               !(cue_id %in% fillers_omit)), 
          family="binomial")
summary(fit.EN_US_filler)
fit.EN_US_filler.anova = Anova(fit.EN_US_filler, type = 3)

fit.EN_US_filler.anova
```

```{r VN responding predicted by each corpus model raw co-occurrences for fillers, warning=FALSE, include = F}
# Vietnamese
fit.VI_VN_filler = glmer(responses_word1 ~ word1_match_frequency_prop_V + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "Vietnam", 
                               !(cue_id %in% fillers_omit)), 
          family="binomial")
summary(fit.VI_VN_filler)
fit.VI_VN_filler.anova = Anova(fit.VI_VN_filler, type = 3)

fit.VI_VN_filler.anova
```
Yes, corpus raw co-occurrences predicts responding for both US and Vietnamese. *need to discuss with Shan to incorporate Mandarin due to a lot of empty counts*

#### Cosine distance
```{r US responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# English 
fit.EN_US_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_E + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "US"), 
          family="binomial")
summary(fit.EN_US_filler_cos)
fit.EN_US_filler_cos.anova = Anova(fit.EN_US_filler_cos, type = 3)

fit.EN_US_filler_cos.anova
```

```{r VN responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# Vietnamese
fit.VI_VN_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_V + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "Vietnam"), 
          family="binomial")
summary(fit.VI_VN_filler_cos)
fit.VI_VN_filler_cos.anova = Anova(fit.VI_VN_filler_cos, type = 3)

fit.VI_VN_filler_cos.anova
```

```{r CN responding predicted by each corpus model cosine distances for fillers, warning=FALSE, include = F}
# Vietnamese
fit.ZH_CN_filler_cos = glmer(responses_word1 ~ word1_match_cosine_prop_M + (1 | subject) + (1 | cue), 
          data = df.filler %>% filter(country == "China"), 
          family="binomial")
summary(fit.ZH_CN_filler_cos)
fit.ZH_CN_filler_cos.anova = Anova(fit.ZH_CN_filler_cos, type = 3)

fit.ZH_CN_filler_cos.anova
```
Corpus cosine similarities also predict responding for all cultures. 

# Discussion
In this paper, we consider whether statistics of the language environment can account for cross-cultural differences in a classic similarity judgment paradigm, as an alternative to the view that members of different cultures vary in their conception of similarity. 

We first tested the generality of a cultural account which holds that people from Western and East Asian cultures tend to conceive of similarity in more taxonomic and thematic ways, respectively, and respond accordingly in categorization tasks such as ours. While we managed to replicate the previously documented contrast between English speakers in the US and Mandarin Chinese speakers from East Asia (mainland China, Taiwan, Hong Kong, and Singapore), we do not extend this contrast to our sample of Vietnamese speakers in Vietnam and English speakers in the US. This finding suggests some limitations on the generality of this cultural account. 

We did find some signatures of language specificity in our analysis, such as the large positive correlation between similarity judgments of each country and the respective corpus statistics, and how each corpus statistics are good predictors for corresponding country's similarity judgments. However, this is potentially due to the high correlation between corpus statistics of English, Vietnamese and Mandarin. We find even stronger evidence for consistency across the three groups, with substantive overlapping predictions across the corpus models, highly similar responding across the experiments, and a correspondingly high fit in cross-language comparisons between models and data.

```{r include=FALSE, cache = TRUE}
fit.country_no_triad <- glmer(responses_theme ~ country + (1 | subject), 
                              data = df_main_analysis, 
                              family = "binomial")

fit.country = glmer(responses_theme ~ country + (1 | subject) + (country | triad), 
                    data = df_main_analysis, 
                    family = "binomial")
summary(fit.country)

fit.no_triad_compare <- anova(fit.country, fit.country_no_triad, type = 3)
```

There are some suggestions that the current approach can predict triad-specific cross-cultural effects. First of all, an ANOVA model comparison showed that adding a random effect term for varying slope by country and varying intercept by triad to the model produces a significant better fit than the identical model without this random effect term included as a predictor ($\chi^2$(`r fit.no_triad_compare["fit.country", "Df"]`) = `r round(fit.no_triad_compare["fit.country", "Chisq"], 2)`, p `r ifelse( fit.no_triad_compare["fit.country", "Pr(>Chisq)"] > 0.001, paste0("= ", round(fit.no_triad_compare["fit.country", "Pr(>Chisq)"], 3)), "< .001")`)

<!-- Anecdotally, we show below some examples of triads that show contrasting difference in the magnitude of difference in thematic responses between countries. As a review, the overall result across all triads showed that there was significant difference between US and China responding, and no significant differences between US and Vietnam or China and Vietnam responding. However, we observed that for the triad "cow-grass-chicken," there were significant differences between US and China, and US and Vietnam responding. There was no significant difference between China and Vietnam responding. For the triad "spoon-sugar-fork," there were significant difference between China and US, and China and Vietnam responding, but no significant difference between US and Vietnam. For the triad "hair-comb-beard," there were no significant differences pairwise across the three cultural contexts. This suggests that there are triad-specific cross-cultural effects in play. Beyond emphasizing the importance of including triad predictors as a random effect, this finding suggests that, firstly, future similar studies need to report triads that are used, as the triads used can affect results (for example, if a study only includes triads where China and US responding are significantly different, it would be more likely to conclude that Chinese and US cultures hold different notions of similarity). Secondly, that future studies can look more deeply into which groups of triads are driving the US-China responding difference. For example, triads belonging to different semantic neighborhoods might be influencing cross-cultural responding in different ways. -->

```{r include=FALSE, eval = F, cache = TRUE}
df.country_item <- df_main_analysis %>%
  filter(triad == "hair")

plot.hair <- ggplot(df.country_item,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = responses_theme, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_jitter(stat = "identity", alpha  = .5) +
  labs(y = "Proportion Thematic Chosen (hair: beard / comb)",  
       x = "Country", 
       color = "Country") + 
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  scale_y_continuous(limits=c(0.0, 1.0), oob = scales::squish) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237")) +  
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237"))  + 
  theme(axis.title = element_text(size=rel(0.75)))
```

```{r include=FALSE, eval = F, cache = TRUE}
df.country_item <- df_main_analysis %>%
  filter(triad == "spoon2")

plot.spoon2 <- ggplot(df.country_item,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = responses_theme, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_jitter(stat = "identity", alpha  = .5) +
  labs(y = "Proportion Thematic Chosen (spoon: fork / sugar)",  
       x = "Country", 
       color = "Country") + 
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  scale_y_continuous(limits=c(0.0, 1.0), oob = scales::squish) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237")) +  
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237")) + 
  theme(legend.position = "none") + 
  theme(axis.title = element_text(size=rel(0.75)))
```

```{r include = FALSE, eval = F, cache = TRUE}
#show 1 triad

df.country_item <- df_main_analysis %>%
  filter(triad == "cow2")

df.country_item_summ <- df %>%
  group_by(triad, country) %>%
  summarize(theme_resp_percent = mean(responses_theme, na.rm = T)) %>%
  filter(triad == "cow2")

plot.cow2 <- ggplot(df.country_item,
       mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
                     y = responses_theme, 
                     color = factor(country, levels=c("China", "US", "Vietnam")))) +
  geom_jitter(stat = "identity", alpha  = .5) +
  labs(y = "Proportion Thematic Chosen (cow: chicken / grass)",  
       x = "Country", 
       color = "Country") + 
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "pointrange") + 
  scale_y_continuous(limits=c(0.0, 1.0), oob = scales::squish) + 
  scale_fill_manual(values=c("#D63230", "#1C77C3", "#F39237")) +  
  scale_color_manual(values=c("#D63230", "#1C77C3", "#F39237")) + 
  theme(legend.position = "none") + 
  theme(axis.title = element_text(size=rel(0.75)))
# 
# +  
#   geom_bar(data = df.country_item_summ, 
#            mapping = aes(x = factor(country, levels=c("China", "US", "Vietnam")), 
#                           y = theme_resp_percent, 
#                          color = factor(country, levels=c("China", "US", "Vietnam"))), 
#            stat='identity', alpha = .3, 
#            fill=c("#D63230", "#1C77C3", "#F39237"))
```

```{r echo=FALSE, eval = F, warning = FALSE, cache = TRUE, fig.width=8, fig.cap="Anecdotal evidence of triad-specific effects. Cow-grass-chicken shows significant differences between US-China responding, and between US-Vietnam responding. Spoon-sugar-fork shows only a significant difference between US-China responding. Hair-comb-beard does not show any significant differences in pairwise comparisons across the three countries."}
plot.cow2 + plot.spoon2 + plot.hair + 
    plot_layout(ncol = 3)
```

Our findings raise additional questions for future work: if not differences in taxonomic vs. thematic responding, then what differences drive the relativity effects previous studies have observed? To what extent are the relativity effects driven by language, and to what extent by culture? @Ji2004 established that culture-aligned differences in this paradigm exist, even when the test language is held constant, concluding that “it is culture (independent of the testing language) that led to different grouping styles” in their study. Our data provide a cautionary note to this conclusion, suggesting that semantic representations in bilinguals (see @Francis2005 for a review) may have the potential to provide an offline account for cross-context differences in similarity judgments, independent of test language. However, there are still many open questions for this account. How do semantic associations guide categorization? Can they explain taxonomic-thematic differences of the type reported by @Ji2004 and others? Can we provide a more specific computational account than the simple frequency model tested here?

Despite these caveats, our findings here demonstrate the plausibility of an alternative perspective on cross-cultural accounts of language, thought, and similarity in the case of taxonomic and thematic reasoning: that it may be the input to similarity judgments, rather than the evaluative process or the conceptualization of similarity that produces variation in similarity reasoning across cultural and linguistic contexts. We hope this work provides a foundation for further research probing this question. 


\newpage


## Two-column images

You can read local images using png package for example and plot 
it like a regular plot using grid.raster from the grid package. 
With this method you have full control of the size of your image. **Note: Image must be in .png file format for the readPNG function to work.**

You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`.

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}
img <- png::readPNG("figs/walrus.png")
grid::grid.raster(img)
```

## One-column images

Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`.

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."}
img <- png::readPNG("figs/lab_logo_stanford.png")
grid::grid.raster(img)
```


## R Plots

You can use R chunks directly to plot graphs. And you can use latex floats in the
fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)**

```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" }
x <- 0:100
y <- 2 * (x + rnorm(length(x), sd = 3) + 3)

ggplot2::ggplot(data = data.frame(x, y), 
                aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


## Tables

Number tables consecutively; place the table number and title (in
10 point) above the table with one line space above the caption and
one line space below it, as in Table 1. You may float
tables to the top or bottom of a column, set wide tables across both
columns.

You can use the xtable function in the xtable package.

```{r xtable, results="asis"}
n <- 100
x <- rnorm(n)
y <- 2*x + rnorm(n)
out <- lm(y ~ x)

tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), 
                       caption = "This table prints across one column.")

print(tab1, type="latex", comment = F, table.placement = "H")
```

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

# Appendix
## 1. ICC of item variability and participant variability 

```{r}
US_agr <- agreement::dim_icc(df_main_analysis %>% filter(country == "US"),
                             model = "2A",
                             type = "agreement",
                             unit = "average",
                             object = triad,
                             rater = subject,
                             score = responses_theme,
                             bootstrap = 0)

CN_agr <- agreement::dim_icc(df_main_analysis %>% filter(country == "China"),
                             model = "2A",
                             type = "agreement",
                             unit = "average",
                             object = triad,
                             rater = subject,
                             score = responses_theme,
                             bootstrap = 0)

VN_agr <- agreement::dim_icc(df_main_analysis %>% filter(country == "Vietnam"),
                             model = "2A",
                             type = "agreement",
                             unit = "average",
                             object = triad,
                             rater = subject,
                             score = responses_theme,
                             bootstrap = 0)

iccs <- tibble(Sample = c("US","China", "Vietnam"),
               `Intra-rater ICC` = c(US_agr$Intra_ICC,
                                     CN_agr$Intra_ICC,
                                     VN_agr$Intra_ICC),
               `Inter-rater ICC` = c(US_agr$Inter_ICC,
                                     CN_agr$Inter_ICC,
                                     VN_agr$Inter_ICC))
```

```{r item variability, include = F}
df.icc_item_full <- df_main_analysis %>% 
  select(subject, triad, responses_theme) %>%
  pivot_wider(names_from = subject, values_from = responses_theme)

icc_item_full <- icc(df.icc_item_full %>% select(-triad), model = "twoway", type = "consistency", unit = "single")$value

df.icc_item <- df_main_analysis %>% 
  select(subject, triad, responses_theme, country) %>%
  pivot_wider(names_from = subject, values_from = responses_theme)

#unsure if type should be consistency or agreement
df.icc_item_US <- df.icc_item %>% 
  filter(country == "US") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-country, -triad)

icc_item_US <- icc(df.icc_item_US, model = "twoway", type = "agreement", unit = "average")$value

#unsure if type should be consistency or agreement
df.icc_item_VN <- df.icc_item %>% 
  filter(country == "Vietnam") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-country, -triad)

icc_item_VN <- icc(df.icc_item_VN, model = "twoway", type = "consistency", unit = "single")$value

#unsure if type should be consistency or agreement
df.icc_item_CN <- df.icc_item %>% 
  filter(country == "China") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-country, -triad)

icc_item_CN <- icc(df.icc_item_CN, model = "twoway", type = "consistency", unit = "single")$value
```

```{r participant variability, include = F}
df.icc_ppt_full <- df_main_analysis %>% 
  select(subject, triad, responses_theme) %>%
  pivot_wider(names_from = triad, values_from = responses_theme)

icc_ppt_full <- icc(df.icc_ppt_full %>% select(-subject), model = "twoway", type = "consistency", unit = "single")$value

df.icc_ppt <- df_main_analysis %>% 
  select(subject, triad, responses_theme, country) %>%
  pivot_wider(names_from = triad, values_from = responses_theme)

#unsure if type should be consistency or agreement
df.icc_ppt_US <- df.icc_ppt %>% 
  filter(country == "US") %>%
  select(-country, -subject)

icc_ppt_US <- icc(df.icc_ppt_US, model = "twoway", type = "consistency", unit = "single")$value

#unsure if type should be consistency or agreement
df.icc_ppt_VN <- df.icc_ppt %>% 
  filter(country == "Vietnam") %>%
  select(-country, -subject)

icc_ppt_VN <- icc(df.icc_ppt_VN, model = "twoway", type = "consistency", unit = "single")$value

#unsure if type should be consistency or agreement
df.icc_ppt_CN <- df.icc_ppt %>% 
  filter(country == "China") %>%
  select(-country, -subject)

icc_ppt_CN <- icc(df.icc_ppt_CN, model = "twoway", type = "consistency", unit = "single")$value
```

```{r}
icc_tbl <- data.frame(
  Country = c("US", "China", "Vietnam", "All"),
  Item_Variability_ICC = c(icc_item_US, icc_item_CN, icc_item_VN, icc_item_full), 
  Ppt_Variabiity_ICC = c(icc_ppt_US, icc_ppt_CN, icc_ppt_VN, icc_ppt_full)
)
kable(icc_tbl)
```

## 2. CCRR agreement
```{r load CCRR data}
df.ccrr <- read.csv("../../../data/replication/ccrr.csv") %>%
  filter(task_name == "TD") %>%
  select(subject, trial_info, resp) %>%
  rename(triad = trial_info) %>%
  mutate(response_theme = ifelse(resp == "FALSE", 1, 0)) %>%
  select(subject, triad, response_theme) %>%
  pivot_wider(names_from = subject, values_from = response_theme) %>%
  select(-triad)

df.ccrr %>%
    dplyr::group_by(triad, subject) %>%
    dplyr::summarise(n = dplyr::n(), .groups = "drop") %>%
    dplyr::filter(n > 1L)

icc(df.ccrr, model = "twoway", type = "consistency", unit = "single")
```
